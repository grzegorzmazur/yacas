%% LyX 1.3 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{llncs}
%\usepackage[T1]{fontenc}
%\usepackage[latin1]{inputenc}
\usepackage{amsmath}

\makeatletter
\usepackage{babel}
\makeatother
\begin{document}

\title{Arbitrary-precision computation of Chebyshev polynomials}


\author{Serge Winitzki}


\institute{Department of Physics, Ludwig-Maximilians University, 80333 Munich,
Germany}

\maketitle
\begin{abstract}
I describe and compare the main methods for arbitrary-precision numerical
computation of Chebyshev polynomials $T_{n}\left(x\right)$, $U_{n}\left(x\right)$
for complex $x$. I indicate the asymptotically most efficient methods in
the limits of large order $n$ and large required precision $P$.
For $n \gg P$ it is better to use trigonometric relations,
while for $n<P$ the algebraic relation is more efficient.
\end{abstract}

\section{Introduction}

The Chebyshev polynomials of the first and second kind may be defined
by \cite{AS64}\begin{align}
T_{n}\left(\cos\phi\right) & \equiv\cos n\phi,\label{eq:def-T}\\
U_{n}\left(\cos\phi\right) & \equiv\frac{\sin\left(n+1\right)\phi}{\sin\phi}.\label{eq:def-U}\end{align}
The polynomials $T_{n}\left(x\right)$ and $U_{n}\left(x\right)$
are of degree $n$; the first polynomials are $T_{0}\left(x\right)=U_{0}\left(x\right)=1$
and $T_{1}\left(x\right)=x,$ $U_{1}\left(x\right)=2x$.

I consider the problem of computing the numerical values $T_{n}\left(x\right)$
and $U_{n}\left(x\right)$ to the relative precision of $P$ digits,
assuming that $x$ is a complex number known exactly or approximated
to arbitrary accuracy. The Chebyshev polynomials satisfy numerous
identities and can be computed by a variety of methods. It is interesting
to analyze the asymptotic case when both $n$, the order of the polynomials,
and the required precision $P$ are large (but not necessarily of
the same order of magnitude).

Arbitrary-precision arithmetic facilities are widely available (e.g.~\cite{GMP03}).
I denote by $M\left(P\right)$ the cost of multiplying two $P$-digit
numbers. The asymptotically fastest algorithms take $M\left(P\right)=O\left(P\ln P\ln\ln P\right)$
operations but are cost-effective only at precisions above $\sim10^{4}$
digits; at lower precision the cost can be estimated by a power law
$M\left(P\right)\sim P^{\alpha}$ with $1<\alpha<2$. The asymptotic
cost of computing square roots, exponential and trigonometric functions
to $P$ digits is $O\left(M\left(P\right)\ln P\right)$ for large
$P$ (see e.g.~\cite{Brent76}), while for moderate $P$ the optimal
algorithms for trigonometric functions require $O\left(M\left(P\right)P^{1/3}\right)$
operations \cite{Smith89}. An integer power $x^{n}$ is computed
by the well-known binary algorithm \cite{Knuth81} in $O\left(M\left(P\right)\log_{2}n\right)$
operations. These asymptotic estimates are used to obtain the cost
of evaluating a Chebyshev polynomial.

The target precision $P$ is assumed in \emph{decimal} digits. The
conversion to a non-decimal base is made straightforward by the presence
of explicit base $10$ logarithms in the formulae. 


\section{Available methods}

In the following sections I describe several methods for computing
the Chebyshev polynomials and compare their asymptotic cost.

The Chebyshev polynomials satisfy the following symmetry property,\[
T_{n}\left(-x\right)=\left(-1\right)^{n}T_{n}\left(x\right),\quad U_{n}\left(-x\right)=\left(-1\right)^{n}U_{n}\left(x\right).\]
Therefore it is sufficient to consider the values $x$ such that $\textrm{Re}\, x\geq0$. 


\subsection{Trigonometric formulae}

For real $x$ such that $\left|x\right|<1$, Eqs.~(\ref{eq:def-T})-(\ref{eq:def-U})
are rewritten as

\begin{align}
T_{n}\left(x\right) & =\cos\left[n\arccos x\right],\label{eq:Tcos}\\
U_{n}\left(x\right) & =\frac{\sin\left[\left(n+1\right)\arccos x\right]}{\sqrt{1-x^{2}}}=\sqrt{\frac{1-\left[T_{n+1}\left(x\right)\right]^{2}}{1-x^{2}}}.\label{eq:Usin}\end{align}
 The computation according to these formulae requires an extended-precision
evaluation of $y\equiv\arccos x$, of $\cos ny$ and of square roots.
For real $\left|x\right|<1$ the computation of $\cos ny$ leads to
a loss of precision because of the periodicity of the cosine. To obtain
$P$ digits of $\cos ny$, one needs to know $y$ with at least $P+\log_{10}\left|ny\right|$
digits of relative accuracy. Therefore $x$ must be known to at least
the same precision.

The general case of complex $x$ presents the same problem. An equivalent
form of Eqs.~(\ref{eq:Tcos})-(\ref{eq:Usin}) is \begin{alignat}{1}
T_{n} & =\cosh\left[n\,\textrm{arccosh}\, x\right]=\cosh\left[n\ln\left(x+\sqrt{x^{2}-1}\right)\right],\label{eq:Tcosh1}\\
U_{n} & =\frac{1}{\sqrt{x^{2}-1}}\sinh\left[\left(n+1\right)\ln\left(x+\sqrt{x^{2}-1}\right)\right].\label{eq:Usinh1}\end{alignat}
(Since $n$ is integer, all branches of the logarithm give the same
results.) The expression $z\equiv x+\sqrt{x^{2}-1}$ is real and positive
only for real $x\geq1$, therefore $\ln z$ has a nonzero imaginary
part for all other $x$. If $\arg z\neq0$, the function\[
\cosh\left(n\ln z\right)=\left|z\right|^{n}\cos\left(n\arg z\right)\]
 exhibits periodic behavior and can be computed to $P$ digits only
if $\arg z$ is found to 
$P+\log_{10}\left|n\arg z\right| \approx P+\log_{10}n$ digits.
(This is too conservative for the case $x\approx1$ considered below.)
If $\left|z\right|\neq1$,
we also need to know $\left|z\right|$ to $P+\log_{10}n$ digits.
The asymptotic cost of computing $\ln z$ for large $n$ and $P$
is therefore
\begin{equation}
O\left[M\left(P+\log_{10}n\right)\ln\left(P+\log_{10}n\right)\right].\label{eq:Cost1}\end{equation}
This is the dominant cost of the computation of $T_{n}\left(x\right)$
since $\cosh\left(n\ln z\right)$ costs only $O\left(M\left(P\right)\ln P\right)$.
%For smaller $P$, the cost is\begin{equation}
%O\left(M\left(P+\log_{10}n\right)\left(P+\log_{10}n\right)^{1/3}\right).\label{eq:Cost1a}\end{equation}


The computational cost of $U_{n}\left(x\right)$ is asymptotically
equivalent to that of $T_{n}\left(x\right)$.


\subsection{Algebraic formulae}

One may rewrite Eqs.~(\ref{eq:Tcosh1})-(\ref{eq:Usinh1}) as\begin{align}
T_{n} & =\frac{1}{2}\left(x+\sqrt{x^{2}-1}\right)^{n}+\frac{1}{2}\left(x+\sqrt{x^{2}-1}\right)^{-n},\label{eq:T0sqrt}\\
U_{n} & =\frac{1}{2\sqrt{x^{2}-1}}\left[\left(x+\sqrt{x^{2}-1}\right)^{n+1}-\left(x+\sqrt{x^{2}-1}\right)^{-n-1}\right].\label{eq:U0sqrt}\end{align}
Again the case $x\approx 1$ is special (there is
a cancellation in the formula for $U_{n}$). 
For other $x$, the calculation of the
$n$-th power can be performed on average with
$\approx\frac{3}{2}\log_{2}n$ multiplications and
incurs a loss of $\log_{10}n$ digits in roundoff error, so the working
precision must be increased by this number of digits. The asymptotic
cost of the computation using Eqs.~(\ref{eq:T0sqrt})-(\ref{eq:U0sqrt})
is therefore \begin{equation}
\approx\frac{3}{2}M\left(P+\log_{10}n\right)\log_{2}n.\label{eq:Cost2}
\end{equation}
Note that complex numbers appear in Eqs.~(\ref{eq:T0sqrt})-(\ref{eq:U0sqrt})
unless $x$ is real and $\left|x\right|>1$.


\subsection{The case of $x\approx1$}

The point $x=1$ is a branching point of the inverse cosine. We have
$T_{n}\left(\pm1\right)=1$ and $U_{n}\left(\pm1\right)=\pm\left(n+1\right)$.

Suppose $x=1+q\cdot10^{-Q}$, where $Q\geq1$ and $q$ is a complex
number with $10^{-1}<\left|q\right|\leq1$. The number $z=x+\sqrt{x^{2}-1}$
is also close to $1$ since\begin{equation}
x^{2}-1=q\cdot10^{-Q}\left(2+q\cdot10^{-Q}\right)\end{equation}
 is small and \begin{equation}
z\approx1+10^{-Q/2}\sqrt{2q}.\end{equation}
 To find $P+\log_{10}n$ digits of $z$, one needs to know $q$ with
$P+\log_{10}n-Q/2$ digits (relative accuracy). One computes $\ln z$ (a small number)
to $P+\log_{10}n-Q/2$ digits using the Taylor series and then evaluates $\cosh\left(n\ln z\right)$
to $P$ digits. In this way the loss of precision can be avoided.
The total cost is \begin{equation}
O\left[M\left(P\right)\ln P\right]+O\left[M\left(P+\log_{10}n-Q/2\right)\ln\left(P+\log_{10}n-Q/2\right)\right].\end{equation}
 If $Q>2\log_{10}n$ or equivalently $n^{2}\left|1-x\right|<1$, then
$T_{n}\left(x\right)\approx1$ and $U_{n}\left(x\right)\approx n+1$
and the cost of computation is $O\left(M\left(P\right)\ln P\right)$.
Otherwise $P+\log_{10}n-Q/2>P$ and the cost is only slightly smaller
than that given by Eq.~(\ref{eq:Cost1}).


\subsection{Linear recurrences}

The Chebyshev polynomials satisfy linear recurrences\begin{equation}
T_{n+2}\left(x\right)=2xT_{n}\left(x\right)-T_{n-1}\left(x\right).\end{equation}
A straightforward computation would require $O\left(n\right)$ long
multiplications. But the matrix form of the same relation,\begin{eqnarray}
\left(\begin{array}{c}
T_{n+1}\\
T_{n}\end{array}\right) & = & \left[\begin{array}{cc}
2x & -1\\
1 & 0\end{array}\right]^{n}\left(\begin{array}{c}
x\\
1\end{array}\right),\\
\left(\begin{array}{c}
U_{n+1}\\
U_{n}\end{array}\right) & = & \left[\begin{array}{cc}
2x & -1\\
1 & 0\end{array}\right]^{n}\left(\begin{array}{c}
2x\\
1\end{array}\right),\end{eqnarray}
 reduces the cost to $O\left(\log_{2}n\right)$ multiplications if
the fast algorithm is used for matrix powers. Each squaring loses
one bit of precision, therefore the working precision must be increased
by $\log_{10}n$ digits. The asymptotic cost is given by Eq.~(\ref{eq:Cost2}).
The actual number of required operations is several times higher than
that in Eqs.~(\ref{eq:T0sqrt})-(\ref{eq:U0sqrt}) because of the matrix
multiplication overhead. Therefore
the linear recurrence method offers no practical advantage.


\subsection{Quadratic recurrences and Fibonacci sequences}

The following relations can be used \cite{AS64}, \begin{align}
T_{a+b}\left(x\right) & =2T_{a}\left(x\right)T_{b}\left(x\right)-T_{a-b}\left(x\right),\label{eq:Tquad}\\
U_{a+b}\left(x\right) & =2U_{a}\left(x\right)T_{b}\left(x\right)-U_{a-b}\left(x\right).\label{eq:Uquad}\end{align}
 These relations recursively reduce the computation of $T_{n}\left(x\right)$,~$U_{n}\left(x\right)$
to the same computation for smaller numbers $n$ until we reach $T_{n}$,~$U_{n}$
for $n=1$ or $n=0$ that do not require any work. 

To avoid recursion, one can build a sequence of indices $k_{1}$,
$k_{2}$, ..., $n$ such that the value $T_{k_{i}}\left(x\right)$ for each
subsequent element $k_{i}$ can be computed using the recurrences
and previously found values $T_{k}\left(x\right)$. For example,
the sequence (2, 3, 5, 10, 15) directs one to first compute $T_{2}\left(x\right)$
using the recurrence with $a=b=1$, then $T_{3}\left(x\right)$ using
$a=2$ and $b=1$, then $T_{5}\left(x\right)$, $T_{10}\left(x\right)$,
and finally $T_{15}\left(x\right)$. If a sequence has length $L$,
the calculation requires $L$ consecutive multiplications. The roundoff
error is $\log_{10}n$ digits, independently of the choice of the
sequence. (The largest power of $x$ in the result is always $x^{n}$
which incurs the loss of $\log_{10}n$ digits.) Therefore, the asymptotic
cost is \begin{equation}
LM\left(P+\log_{10}n\right).\end{equation}


The performance of this method depends on the length of the sequence
($k_{1}=2$, ..., $k_{L}=n$). One method to build the sequence was
used in Ref.~\cite{Koepf99}. This method employs recurrences only
with $a=b$ or $a=b+1$. For example, to compute $T_{39}\left(x\right)$
one divides $39$ by $2$ and uses the recurrence with $a=20$ and
$b=19$. Then one divides $19$ by $2$ and uses $a=10$ and $b=9$,
etc. The resulting chain is (2, 3, 4, 5, 9, 10, 19, 20, 39). For
a given $n$ the bisection chain can be easily generated by examining the binary
representation of $n$. The bisection method gives 
sequences with $L\approx2\log_{2}n$
elements and the cost is $4/3$ of that given by Eq.~(\ref{eq:Cost2}).

The sequence obtained using bisection is not the most optimal one.
For example, for $n=39$ there is a shorter sequence (2, 3, 6,
9, 15, 24, 39) that is reminiscent of the Fibonacci sequence because
many elements are sums of the two previous elements. Exhaustive search for
optimal sequences for all $n$ up to $n=4000$ gives on average $L_{\textrm{opt}}\left(n\right)\approx1+\log_{1.58}n$.
The coefficient $1.58$ is approximately the same as that in Eq.~(\ref{eq:Cost2}).
Therefore the method of quadratic recurrences does not improve
on the performance or roundoff error of the
algebraic method even if optimal sequences are used.

\subsection{Explicit coefficients}

The coefficients of the Chebyshev polynomials can be obtained by explicit
formulae. If $k=\left\lfloor \frac{n}{2}\right\rfloor $ and $n>0$,
then \begin{align}
T_{n}\left(x\right) & =\sum_{i=0}^{k}\left(-1\right)^{i}\frac{\left(2x\right)^{n-2i}}{n-i}{{n-i \choose i}},\label{eq:Texplicit}\\
U_{n}\left(x\right) & =\sum_{i=0}^{k}\left(-1\right)^{i}\left(2x\right)^{n-2i}{{n-i \choose i}}.\label{eq:Uexplicit}\end{align}
 The summation is over integer values of $i$ such that $0\leq2i\leq n$,
regardless of whether $n$ is even or odd.

Computed straightforwardly, Eqs.~(\ref{eq:Texplicit})-(\ref{eq:Uexplicit})
require $O\left(M\left(P\right)n\right)$ operations. To avoid the
accumulation of the roundoff error during $n$ arithmetic operations,
one needs about $\log_{10}n$ additional digits of working precision.
In Eqs.~(\ref{eq:Texplicit})-(\ref{eq:Uexplicit}), the next coefficients
can be obtained from the previous ones by a few short multiplications
and divisions. Therefore, a faster method can be used \cite{Smith89}
that reduces the cost to \begin{equation}
O\left(M\left(P+\log_{10}n\right)\sqrt{n}\right).\label{eq:Cost3}\end{equation}


It is clear that for large $n$ and $P$ the present method is inferior
to any of the methods with the cost of Eq.~(\ref{eq:Cost2}). At
very small $n$ this method could outperform other methods due to
its lower overhead (no square roots, logarithms or trigonometric functions
are needed). The precise threshold needs to be measured in particular
implementations.


\section{Conclusions}

I obtained explicit cost estimates for all major methods of arbitrary-precision
computation of the Chebyshev polynomials. The most efficient methods
fall into two categories: one described by the cost estimate of Eq.~(\ref{eq:Cost1})
and the other by Eq.~(\ref{eq:Cost2}). It is found by inspection
that the lowest asymptotic cost is given by Eq.~(\ref{eq:Cost1})
for very large $n\gg P$ and by Eq.~(\ref{eq:Cost2}) for very large
$P\gg n$. However, the threshold values of $n$ and $P$ cannot be
determined without considering a particular implementation of the
methods. The methods of linear and quadratic recurrences have the
same asymptotic cost {[}Eq.~(\ref{eq:Cost2}){]} 
but are inferior to the trigonometric and algebraic formulae.
The choice of the
best method for given $n$ and $P$ requires system-specific benchmarks.

I am grateful to Matthew Parry for helpful discussions.

\begin{thebibliography}{1}
\bibitem{AS64}M. Abramowitz and I. Stegun, eds., \emph{Handbook of special functions}
(National Bureau of Standards, Washington, D.C., 1964).
\bibitem{GMP03}T. Granlund. \emph{GNU MP: The GNU multiple-precision arithmetic library.}
Version 4.1.2 (2003). Available from \texttt{http://swox.com/gmp/}.
\bibitem{Brent76}R. P. Brent, {}``Fast multiple-precision evaluation of elementary
functions'', J. ACM \textbf{23}, p. 242 (1976).
\bibitem{Smith89}D. M. Smith, {}``Efficient multiple-precision evaluation of elementary
functions'', Math. Comp. \textbf{52}, p. 131 (1989).
\bibitem{Knuth81}D. Knuth, \emph{The art of computer programming,} ch.~4 (2nd ed.,
Addison-Wesley, 1981).
\bibitem{Koepf99}W. Koepf, {}``Efficient computation of Chebyshev polynomials'',
in: M. Wester (Ed.), \emph{Computer Algebra Systems: A Practical Guide}
(Wiley, Chichester, 1999), p. 79.\end{thebibliography}

\end{document}
