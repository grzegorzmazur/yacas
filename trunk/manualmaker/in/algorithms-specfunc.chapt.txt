		Euler's Gamma function

Euler's Gamma function $Gamma(z)$ is defined for complex $z$ such that
$Re(z)>0$ by the integral
$$Gamma(z):=Integrate(z, 0, Infinity) Exp(-t)*t^(z-1)$$.
The Gamma function satisfies several identities that can be proved by
rearranging this integral; for example, $Gamma(z+1)=z*Gamma(z)$. This identity
defines $Gamma(z)$ for all complex $z$. The Gamma function is regular
everywhere except nonpositive integers (0, -1, -2, ...) where it diverges.

For real integers $n>0$, the Gamma function is the same as the factorial, 
$$ Gamma(n+1) := n! $$,
so the factorial notation can be used for the Gamma function too. Some formulae
become a little simpler when written in factorials.

The Gamma function is implemented as {Gamma(x)}. At integer values {n} of the
argument, {Gamma(n)} is computed exactly. For half-integer values it is
also computed exactly, using the following identities (here $n$ is a
nonnegative integer):
$$ (+(2*n+1)/2)! = Sqrt(Pi)*(2*n+1)! / (2^(2*n+1)*n!) $$,
$$ (-(2*n+1)/2)! = (-1)^n*Sqrt(Pi) * (2^(2*n)*n!) / (2*n)! $$.
For efficiency, "staggered factorials" are used in this calculation.

For arbitrary complex arguments with nonnegative real part, the library
function {GammaNum(x)} computes a uniform appoximation of Lanczos and Spouge
(with the so-called "less precise coefficients of Spouge"). See: C. J. Lanczos,
J. SIAM of Num. Anal. Ser. B, vol. 1, 86 (1964); J. L. Spouge, J. SIAM of Num.
Anal., vol. 31, 931 (1994). See also: Paul Godfrey 2001 (unpublished):
<*http://winnie.fit.edu/~gabdo/gamma.txt*> for some explanations on the
method.  The method gives the $Gamma$-function only for arguments with positive
real part; at negative values of the real part of the argument, the
$Gamma$-function is computed via the identity
$$Gamma(x)*Gamma(1-x) = Pi/Sin(Pi*x)$$.

The approximation formula used in Yacas depends on a parameter $a$,
$$Gamma(z) = (Sqrt(2*Pi)*(z+a)^(z-1/2)) / (z*e^(z+a)) * (1+e^(a-1)/Sqrt(2*Pi) * Sum(k, 1, N, c[k]/(z+k)))$$,
with $N := Ceil(a)-1$. The coefficients $c[k]$ are defined by
$$ c[k] = (-1)^(k-1)*(a-k)^(k-1/2)/(e^(k-1)*(k-1)!) $$.
The parameter $a$ is a free parameter of the approximation that determines also
the number of terms in the sum. Some choices of $a$ may lead to a slightly more
precise approximation, but larger $a$ is always better. The number of terms $N$
must be large enough to produce the required precision. The estimate of the relative error for
this formula is valid for all $z$ such that $Re(z)>0$ and is
$$error < (2*Pi)^(-a)/Sqrt(2*Pi*a)*(a/(a+z))$$.
The lowest value of $a$ to produce $P$ correct digits is estimated as
$$a = (P-Ln(P)/Ln(10)) * (Ln(10)/Ln(2*Pi))-1/2$$. In practical calculations, the integer logarithm routine {IntLog} is used and the constant $Ln(10)/Ln(2*Pi)$ is approximated from above by 659/526, so that $a$ is not underestimated.

The coefficients $c[k]$ and  the parameter $a$ can be chosen to
achieve a greater precision of the approximation formula. However, the recipe
for the coefficients $c[k]$ given in the paper by Lanczos is too complicated
for practical calculations in arbitrary precision: the time it would take to
compute the array of $N$ coefficients $c[k]$ grows as $N^3$. Therefore it is
better to use less precise but much simpler formulae derived by Spouge.

In the calculation of the sum $Sum(k, 1, N, c[k]*(z+k)^(-1))$, round-off error
can lead to a serious loss of precision. At version
*EVAL Version() : ","
{Yacas} is limited in
its internal arbitrary precision facility in that does not support
floating-point numbers with mantissa; this hinders precise calculations with
floating-point numbers. (This concern does not apply to Yacas linked with
{gmp}.) In the current version of the {GammaNum()} function, two workarounds
are implemented. First, a Horner scheme is used to compute the sum; this is somewhat
faster and leads to smaller roundoff errors. Second, intermediate calculations
are performed at 40{%} higher precision than requested. This is much slower but
allows to obtain results at desired precision.

If the factorial of a large integer or half-integer $n$ needs to be computed not exactly but only with a certain floating-point precision, it is faster (for large enough $Abs(n)$) not to evaluate an exact integer product, but to use the {GammaNum} approximation or Stirling's asymptotic formula,

$$Ln(n!) <=> Ln(2*Pi*n)/2+n*Ln(n/e)+1/(12*n)-1/(360*n^3)+...$$
(The coefficients of the series expansion are related to Bernoulli numbers: $B[k+1]/(k*(k+1)*n^k)$ for $k>=1$.)
This method is currently not implemented in Yacas.

		Riemann's Zeta function

Riemann's Zeta function $zeta(s)$ is defined for complex $s$ such that $Re(s)>0$ as a sum of inverse powers of integers:
$$zeta(s) := Sum(n,0,Infinity, 1/n^s)$$.
This function can be analytically continued to the entire complex plane except
the point $s=1$ where it diverges. It satisfies several identities, for
example, a formula useful for negative $Re(s)$,
$$zeta(1-s) = (2*Gamma(s))/(2*Pi)^s*Cos((Pi*s)/2)*zeta(s)$$,
and a formula for even integers that helps in numerical testing,
$$zeta(2*n) = ((-1)^(n+1)*(2*Pi)^(2*n))/(2*(2*n)!)*B[2*n]$$,
where $B[n]$ are Bernoulli numbers.

The classic book of Bateman and Erdelyi, <i>Higher Transcendental Functions</i>,
vol. 1, describes many results concerning analytic properties of $zeta(s)$.

For the numerical evaluation of Riemann's Zeta function with arbitrary
precision to become feasible, one needs special algorithms. Recently P. Borwein
gave a simple and quick approximation algorithm for positive $Re(s)$ (P.
Borwein,
<i>An efficient algorithm for Riemann Zeta function</i> (1995),
published online and in Canadian Math. Soc. Conf. Proc., 27 (2000), 29-34.) See
also:
J. M. Borwein, D. M. Bradley, R. E. Crandall:
<i>Computation strategies for the Riemann Zeta function</i>, preprint
CECM-98-118, 1999, for a review of methods.

It is the "third" algorithm (the simplest one) from P. Borwein's paper which is implemented in
Yacas. The approximation formula valid for $Re(s) > -(n-1)$ is
$$ zeta(s) = 1/(2^n*(1-2^(1-s)))*Sum(j,0,2*n-1, e[j]/(j+1)^s) $$,
where the coefficients $e[j]$ for $j=0$, ..., $2*n-1$ are defined by
$$ e[j] := (-1)^(j-1)*(Sum(k,0,j-n, n! /(k! *(n-k)!))-2^n) $$,
and the empty sum (for $j<n$) is taken to be zero. The parameter $n$ must be
chosen high enough to achieve the desired precision. The error estimate for this
formula is approximately
$$error < 8^(-n)$$
for the relative precision, which means that to achieve $P$ correct digits we
must have $n>P*Ln(10)$/$Ln(8)$.

The function {Zeta(s)} calls {ZetaNum(s)} to compute this approximation formula
for $Re(s)>1/2$ and uses the identity above to get the value for other $s$.

For very large values of $s$, it is faster to use another method implemented in the routine {ZetaNum1(s, N)}. If the required precision is $P$ digits and
$s > 1 + Ln(10)/Ln(P)*P$,
then it is enough to compute the defining series for $zeta(n)$,
$$zeta(n) <=> Sum(k,1,N, 1/k^n)$$,
up to a certain number of terms $N$.
The required number of terms $N$ is given by $$N = 10^(P/(s-1))$$.
For example, at 100 digits of precision it is advisable to use
{ZetaNum1(s)} only for $s>50$, since it would require $N<110$ terms in
the series, whereas the expression used in {ZetaNum(s)} uses
$n=Ln(10)/Ln(8)*P$ terms (of a different series).

		Lambert's $W$ function

Lambert's $W$ function is (a multiple-valued, complex function) defined for any (complex) $z$ by
$ W(z) * Exp(W(z)) = z$.
This function is sometimes useful to represent solutions of transcendental equations or integrals.


Asymptotics of Lambert's $W$ function are, as usual, not precise enough for actual calculations:
$$W((z^2-2)/2/Exp(1)) = -1 + z - z^3/3 + 11/72*z^4 - ...$$ (this is only good really close to z=0);
$W(x) = x-x^2+3/2*x^3-...$ (this is good very near x=0);
$$W(x) = Ln(x) - Ln(Ln(x)) + Ln(Ln(x))/Ln(x) + 1/2*(Ln(Ln(x))/Ln(x))^2 + ...$$ (good for very large $x$ but it is not clear how to form higher terms!).

Fortunately, one can use uniform rational approximations:
$$W(x) <=> Ln(1+x)*(1-Ln(1+Ln(1+x))/(2+Ln(1+x)))$$ (uniformly good for $x>0$, relative error not more than $10^(-2)$); and
$$W(x) <=> (x*Exp(1)) / (1+( (2*(x*Exp(1)+1))^(-1/2) -1/Sqrt(2) +1/(Exp(1)-1))^(-1) )$$
(uniformly good for $-Exp(-1)<x<0$, relative error not more than $10^(-3)$).

There is also a uniform approximation of the form
$$ W(x) <=> (a+l)*(f-Ln(1+c*Ln(1+d*y))+2*l)/(a+1/2+l) $$,
where $a$, $b$, $c$, $d$, $f$ are certain constants, $y:=Sqrt(2*x*Exp(1)+2)$ and $l:=Ln(1+b*y)$.
This approximation miraculously works over the whole complex plane within relative error at most $0.008$.
However, the coefficients $a$, ..., $f$ are not elementary constants; they have
to be found numerically by matching to the asymptotics at three points
$x= -Exp(-1)$, $x=0$, $x=Infinity$.


The numerical procedure uses the Newton-Halley iteration and the first two uniform rational approximation of $W(x)$ for the initial value of $x$.

The Halley iteration for the equation $W*Exp(W)=x$ can be written as
$$ W'=W-(W-x*Exp(-W))/(W+1-(W+2)/(W+1)*(W-x*Exp(-W))/2) $$. It has cubic convergence for any initial value $W> -Exp(-1)$.


		Bessel functions


Bessel functions are a family of special functions solving the equation
$$ (Deriv(x,2)w(x))+1/x*(D(x)w(x))+(1-n^2/x^2)*w(x)=0 $$. There are two linearly independent solutions which can be taken as the pair of Hankel functions $H1(n,x)$, $H2(n,x)$, or as the pair of Bessel-Weber functions $J[n]$, $Y[n]$.
These pairs are linearly related,
$J[n]=1/2*(H1(n,x)+H2(n,x))$, $J[n]=1/(2*I)*(H1(n,x)-H2(n,x))$. The function $H2(n,x)$ is the complex conjugate of $H1(n,x)$. This arrangement of four functions is very similar to the relation between $Sin(x)$, $Cos(x)$ and $Exp(I*x)$, $Exp(-I*x)$, which are all solutions of $(Deriv(x,2)f(x))+f(x)=0$.

For large values of $Abs(x)$, there is the following asymptotic series:
$$ H1(n,x) <> Sqrt(2/(Pi*x))*Exp(I*zeta)*Sum(k,0,Infinity, I^k*A(k,n)/x^k) $$,
where
$zeta:=x-1/2*n*Pi-1/4*Pi$ and
$$ A(k, n) := ( (4*n^2-1^2)*(4*n^2-3^2)*...*(4*n^2-(2*k-1)^2))/(k! *8^k) $$.
From this one can find the asymptotic series for
$ J[n] <> Sqrt(2/(Pi*x))*Cos(zeta)*Sum(k,0,Infinity, (-1)^k*A(2*k,n)*x^(-2*k)) - Sqrt(2/(Pi*x))*Sin(zeta)*Sum(k,0,Infinity, (-1)^k*A(2*k+1,n)*x^(-2*k-1)) $ and
$ Y[n] <> Sqrt(2/(Pi*x))*Sin(zeta)*Sum(k,0,Infinity, (-1)^k*A(2*k,n)*x^(-2*k)) + Sqrt(2/(Pi*x))*Cos(zeta)*Sum(k,0,Infinity, (-1)^k*A(2*k+1,n)*x^(-2*k-1)) $.

The error of a truncated asymptotic series is not larger than the first discarded term if the number of terms is larger than $n-1/2$.
(See the book: F. W. J. Olver, <i>Asymptotic and special functions</i>, Academic Press, 1974.)

Currently Yacas can compute {BesselJ(n,x)} for all $x$ where $n$ is an integer and for $Abs(x)<= 2*Gamma(n)$ when
$n$ is a real number. Yacas currently uses the Taylor series
when $Abs(x)<= 2*Gamma(n)$ to compute the numerical value:
$$ BesselJ(n,x) := Sum(k,0,Infinity,(-1)^k*x^(2*k+n)/(2^(2*k+n)* k! * Gamma(k+n+1))) $$.

If $Abs(x) > 2*Gamma(n)$ and $n$ is an integer, then Yacas uses the forward recursion
identity:
$$ BesselJ(n,x) := (2*(n+1)/x)*BesselJ(n+1,x) - BesselJ(n+2,x) $$
until the given {BesselJ} function is represented in terms of higher order terms which 
all satisfy $Abs(x) <= 2*Gamma(n)$. Note that when $n$ is much smaller than $x$, this
algorithm is quite slow because the number of Bessel function evaluations grows like $2^i$, where $i$ is
the number of times the recurrence identity is used.

We see from the definition that when $Abs(x)<=2*Gamma(n)$, the absolute value
of each term is always decreasing (which is called absolutely monotonely
decreasing.) From this we know that if we stop after $i$ iterations, the error
will be bounded by the absolute value of the next term. So given a set
precision, turn this into a value $epsilon$, so that we can check if the current
term will contribute to the sum at the prescribed precision. Before doing this,
Yacas currently increases the precision by 20{%} to do interim calculations.
This is a heuristic that works, it is not backed by theory.  The value $epsilon$
is given by $epsilon:=5*10^(-prec)$, where $prec$ was the previous precision. This
is directly from the definition of floating point number which is correct to
$prec$ digits: A number correct to $prec$ digits has a rounding error no
greater than $5*10^(-prec)$. Beware that some books incorrectly have $.5$
instead of $5$.

Bug: Something is not right with complex numbers, but pure imaginary are OK.

		Bernoulli numbers and polynomials

The Bernoulli numbers $B[n]$ come from a sequence of rational numbers defined
by the series expansion of the following generating function,
$$z/(e^z-1) = Sum(n, 0, Infinity, B[n]*z^n/n!)$$.
The Bernoulli polynomials $B(x)[n]$ are defined similarly by
$$(z*Exp(z*x))/(e^z-1) = Sum(n, 0, Infinity, B(x)[n]*z^n/n!)$$.
The Bernoulli polynomials are related to Bernoulli numbers by
$$B(x)[n] = Sum(k,0,n,x^k*B[n-k]*Bin(n,k))$$,
where $Bin(n,k)$ are binomial coefficients.

Bernoulli numbers and polynomials are used in various Taylor series expansions, in the Euler-Maclauren series resummation formula, in Riemann's Zeta function and so on. For example, the sum of (integer) $p$-th powers of consecutive integers is given by
$$Sum(k, 0, n-1, k^p) = (B(n)[p+1] - B[p+1])/(p+1)$$.

The Bernoulli polynomials $B(x)[n]$ can be found by first computing an array of Bernoulli numbers up to $B[n]$ and then applying the above formula for the coefficients.

In this definition, the first Bernoulli numbers are $B[0]=1$, $B[1]= -1/2$, $B[2]=4$, $B[3]=0$,  $B[4]= -1/30$.

We consider two distinct computational tasks: evaluate a Bernoulli number exactly as
a rational, or find it approximately to a specified floating-point precision.
There are also two possible problem settings: either we need to
evaluate <i>all</i> Bernoulli numbers $B[n]$ up to some $n$, or we only
need one isolated value $B[n]$ for some $n$. Depending on how large $n$
is, different algorithms need to be used in these cases.

	    Exact evaluation of Bernoulli numbers

In the {Bernoulli()} routine,
Bernoulli numbers are evaluated exactly (as rational numbers) via one of the two algorithms. The first, simpler algorithm ({BernoullliArray()}) uses the recursion relation,
$$B[n] = -1/(n+1)*Sum(k,0,n-1,B[k]*Bin(n+1,k))$$.
This formula requires to know the entire set of $B[k]$ with $k$ up to a given
$n$ to compute $B[n]$. Therefore at large $n$ this algorithm
is a very slow way to compute $B[n]$ if we do not need all other $B[k]$.


Here is an estimate of the cost of {BernoullliArray}. Suppose $P$ is the required number of digits and $M(P)$ is the time to multiply $P$-digit integers. The required number of digits $P$ to store the numerator of $B[n]$ is asymptotically $P <> n*Ln(n)$. At each of the $n$ iterations we need to multiply $O(n)$ large rational numbers by large coefficients and take a GCD to simplify the resulting fractions. The time for GCD is logarithmic in $P$. So the complexity of this algorithm is $O(n^2*M(P)*Ln(P))$ with $P <> n*Ln(n)$. 

For large (even) values of the index $n$, the Bernoulli numbers $B[n]$ are
computed by a more efficient procedure: the integer part and the fractional
part of $B[n]$ are found separately.

First, by the theorem of Clausen -- von
Staudt, the fractional part of ($-B[n]$) is the same as the fractional part of
the sum of all inverse prime numbers $p$ such that $n$ is divisible by $p-1$.
To illustrate the theorem, take $n=10$ with
$B[10]=5/66$. The number $n=10$ is divisible only by 1, 2, 5, and 10; this
corresponds to $p=2$, $3$, $6$ and $11$. Of these, $6$ is not a prime.
Therefore, we exclude 6 and take the sum $1/2 + 1/3 + 1/11 = 61/66$. The
theorem now says that $61/66$ has the same fractional part as $-B[10]$; in
other words, $-B[10]=i+f$ where $i$ is some unknown integer and the fractional
part $f$ is a nonnegative rational number, $0<=f<1$, which is now known to be
$61/66$. Indeed $-B[10]= -1+61/66$. So one can find the fractional part of the Bernoulli number relatively quickly by
just checking the numbers that might divide $n$.

Now one needs to obtain the
integer part of $B[n]$. The number $B[n]$ is positive if $Mod(n,4)=2$ and negative if
$Mod(n,4)=0$. One can use Riemann's Zeta function identity for even integer
values of the argument and compute the value $zeta(n)$ precisely enough so that
the integer part of the Bernoulli number is determined. The required precision
is found by estimating the Bernoulli number using the same identity in which
one approximates $zeta(n)=1$ and uses Stirling's asymptotic formula for the
factorial of large numbers,
$$Ln(n!) <=> Ln(2*Pi*n)/2+n*Ln(n/e)$$.

At such 
large values of the argument $n$, it is feasible to compute the defining series
for $zeta(n)$,
$$zeta(n) <=> Sum(k,1,N, 1/k^n)$$.
This sum is calculated by the routine {ZetaNum1(n, N)}. The remainder of the
sum is of order $N^(-n)$. By simple algebra one obtains a lower bound on $N$,
$$N > n/(2*Pi*e)$$,
for this sum to give enough precision to compute the integer part of the
Bernoulli number $B[n]$.

Alternatively, one can use the infinite product over prime numbers $p[i]$
$$1/zeta(n) <=> Factorize(k,1,M,(1-1/p[k]^s))$$,
where $M$ must be chosen such that the $M$-th prime number $p[M]>N$ and $N$ is chosen as above, i.e. $N > n/(2*Pi*e)$. This formula is realized  by the routine {ZetaNum2(n, N)}. Since only prime numbers $p[i]$ are used, this formula is asymptotically faster than {ZetaNum1(n, N)}. The number of primes up to $N$ is asymptotically
$pi(N) <> N/Ln(N)$ and therefore this procedure is faster by a factor $O(Ln(N)) <> O(Ln(n))$. However, for $n<250$ it is better (with Yacas internal math) to use the {ZetaNum1(n, N)} because it involves fewer multiplications.

For example, let us compute $B[20]$ using this method.
*	1. We find the fractional part, using the fact that 20 is divided only by 1, 2, 4, 5, 10, and 20 and only 2, 3, 5, 11 are prime:
	In> 1/2 + 1/3 + 1/5 + 1/11;
	Out> 371/330;
This number has fractional part equal to 41/330 and it's the same as the fractional part of $-B[20]$. Since $B[20]$ must be negative,
this means that $B[20]$ is of the form $-(X+41/330)$ where $X$ is some positive integer which we now have to find.
*	2. We estimate the magnitude of $Abs(B[20])$ to find the required precision to compute its integer part. We use the identity
$$Abs(B[20])=(2*20!)/(2*Pi)^20*zeta(20)$$,
the Stirling formula
$$Ln(20!) = (Ln(2*Pi)+Ln(20))/2+20*Ln(20/e) <=> 42.3$$,
and also the fact that $zeta(20)=1+2^(-20)+...$ is approximately equal to 1.
We find that the number $B[20]$ has approximately
$$ 1+Ln(Abs(B[20]))/Ln(10) = 1+(Ln(2)+Ln(20!)-20*Ln(2*Pi))/Ln(10) <=> 3.72$$
decimal digits before the decimal point; so a precision of 3 or 4 mantissa digits would surely be enough to compute its integer part.
*	3. We now need to compute $zeta(20)$ to 4 decimal digits. The series
$$zeta(20) <=> Sum(k,1,N, 1/k^20)$$
will have an error of order $N^(-20)$. We need this error to be less than
$10^(-4)$ and this is achieved with $N>2$. Therefore it is enough to compute
the sum up to $N=2$:
	In> N(1+1/2^20)
	Out> 1.0000009536;
*	4. Now we find the integer part of $Abs(B[20])$. This time we do not use Stirling's formula for the factorial but compute the exact factorial.
	In> N( 2*20! /(2*Pi)^20*1.0000009536 )
	Out> 529.1242423667;
(Note the space after the factorial sign -- it is needed for the syntax to parse correctly.) Therefore we know that the integer part of $Abs(B[20])$ is 529.
*	5. Since $B[20] = -(X+41/330)$ and we found that $X=529$, we obtain
	In> -(529+41/330);
	Out> -174611/330;
This is exactly the Bernoulli number $B[20]$.

All these steps are implemented in the routine {Bernoulli1}. The variable {Bernoulli1Threshold} determines the smallest $n$ for which $B[n]$ is to be computed via this routine instead of the recursion relation. Its current value is 20. 

The complexity of {Bernoulli1} is estimated as the complexity of finding all primes up to $n$ plus the complexity of computing the factorial, the power and the Zeta function. Finding the prime numbers up to $n$ by checking all potential divisors up to $Sqrt(n)$ requires $O(n^(3/2)*M(Ln(n)))$ operations with precision $O(Ln(n))$ digits. For the second step we need to evaluate $n!$, $Pi^n$ and $zeta(n)$ with precision of $P=O(n*Ln(n))$ digits. The factorial is found in $n$ short multiplications with $P$-digit numbers (giving $O(n*P)$), the power of $pi$ in $Ln(n)$ long multiplications (giving $O(M(P)*Ln(n))$), and {ZetaNum2(n)} (the asymptotically faster algorithm) requires $O(n*M(P))$ operations. The Zeta function calculation dominates the total cost because $M(P)$ is slower than $O(P)$. So the total complexity of {Bernoulli1} is $O(n*M(P))$ with $P <> n*Ln(n)$.

Note that this is the cost of finding just one Bernoulli number, as opposed to the $O(n^2*M(P)*Ln(P))$ cost of finding all Bernoulli numbers up to $B[n]$ using the first algorithm {BernoulliArray}. If we need a complete table of Bernoulli numbers, then {BernoulliArray} is only marginally (logarithmically) slower. So for finding complete Bernoulli tables, {Bernoulli1} is better only for very large $n$.

	    Approximate calculation of Bernoulli numbers

If Bernoulli numbers do not have to be found exactly but only to a certain floating-point precision $P$ (this is usually the case for most numerical applications), then the situation is rather different. First, all calculations can be performed using floating-point numbers instead of exact rationals. This significantly speeds up the recurrence-based algorithms.

However, the recurrence relation used in {BernoulliArray} turns out to be numerically unstable and needs to be replaced by another (R. P. Brent, "A FORTRAN multiple-precision arithmetic package", ACM TOMS vol. 4, no. 1 (1978), p. 57). Brent's algorithm computes the Bernoulli numbers divided by factorials, $C[n]:=B[2*n]/(2*n)!$ using a (numerically stable) recurrence relation

$$2*C[k]*(1-4^(-k)) = (2*k-1)/(4^k*(2*k)!) - Sum(j, 1, k-1, C[k-j]/(4^j*(2*j)!))$$.

The numerical instability of the usual recurrence relation
$$ Sum(j,0,k-1,C[k-j]/((2*j+1)!)) = (k-1/2)/((2*k+1)!) $$
and the numerical stability of Brent's recurrence are not obvious. Here is one way to demonstrate them.
Consider the usual recurrence (above). For large $k$, the number $C[k]$ is approximately $C[k] <=> 2*(-1)^k*(2*Pi)^(-2*k)$. Suppose we use this recurrence to compute $C[k]$ from previously found values $C[k-1]$, $C[k-2]$, etc. and suppose that we have small relative errors $e[k]$ of finding $C[k]$. Then instead of the correct $C[k]$ we use $C[k]*(1+e[k])$ in the recurrence. Now we can derive a relation for the error sequence $e[k]$ using the approximate values of $C[k]$. It will be a linear recurrence of the form
$$ Sum(j,0,k-1,(-1)^(k-j)*e[k-j]*((2*Pi)^(2*j)/((2*j+1)!)) ) = (k-1/2)/((2*k+1)!) * (2*Pi)^(-2*k) $$.
Note that the coefficients for $j>5$ are very small but the coefficients for $0<=j<=5$ are of order 1. This means that we have a cancellation in the first 5 or so terms that produces a very small number $C[k]$ and this may lead to a loss of numerical precision. To investigate this loss, we find eigenvalues of the sequence $e[k]$, i.e. we assume that $e[k] = lambda^k$ and find $lambda$. If $Abs(lambda)>1$, then a small initial error $e[1]$ will grow by a power of $lambda$ on each iteration and it would indicate a numerical instability.

The eigenvalue of the sequence $e[k]$ can be found approximately for large $k$ if we notice that the recurrence relation for $e[k]$ is similar to the truncated Taylor series for $Sin(x)$. Substituting $e[k] = lambda^k$ into it and disregarding a very small number $(2*Pi)^(-2*k)$ on the right hand side, we find 
$$ Sum(j,0,k-1,(-lambda)^(k-j)*((2*Pi)^(2*j)/((2*j+1)!)) ) <=> lambda^k*Sin((2*Pi)/Sqrt(lambda)) <=> 0 $$,
which means that $lambda=4$ is a solution. Therefore the recurrence is unstable.

By a very similar calculation one finds that the inverse powers of $4$ in Brent's recurrence make the largest eigenvalue of the error sequence $e[k]$ almost equal to $1$ and therefore the recurrence is stable. Brent gives the relative error in the computed $C[k]$ as $O(k^2)$ times the roundoff error in the last digit of precision.

The complexity of Brent's method is given as $O(n^2*P+n*M(P))$ for finding all Bernoulli numbers up to $B[n]$ with precision $P$ digits. This computation time can be achieved if we compute the inverse factorials and powers of $4$ approximately by floating-point routines that know how much precision is needed for each term in the recurrence relation. The final long multiplication by $(2*k)!$ computed to precision $P$ adds $M(P)$ to each Bernoulli number.

The non-iterative method using the Zeta function does not perform much better if a Bernoulli number $B[n]$ has to be computed with significantly fewer digits $P$ than the full $O(n*Ln(n))$ digits needed to represent the integer part of $B[n]$. (The fractional part of $B[n]$ can always be computed relatively quickly.) The Zeta function needs $10^(P/n)$ terms, so its complexity is $O(10^(P/n)*M(P))$ (here by assumption $P$ is not very large so $10^(P/n)<n/(2*Pi*e)$; if $n>P$ we can disregard the power of $10$ in the complexity formula). We should also add $O(Ln(n)*M(P))$ needed to compute the power of $2*Pi$. The total complexity of {Bernoulli1} is therefore $O(Ln(n)*M(P) + 10^(P/n)*M(P))$.

If only one Bernoulli number is required, then {Bernoulli1} is always faster. If all Bernoulli numbers up to a given $n$ are required, then Brent's recurrence is faster for certain (small enough) $n$.

Currently Brent's recurrence is implemented as {BernoulliArray1()} but it is not used by {Bernoulli} because the internal arithmetic is not yet able to correctly compute with floating-point precision.

		Error function $Erf(x)$ and related functions

The error function $Erf(z)$ is defined for any (complex) $z$ by
$$ Erf(z) := (2/Sqrt(Pi))*Integrate(t,0,z)Exp(-t^2) $$.

The complementary error function $Erfc(x)$ is defined for real $x$ as
$$ Erfc(x) := (2/Sqrt(Pi))*Integrate(t,x,Infinity)Exp(-t^2) = 1-Erf(x) $$.

Numerical computation of the error function $Erf(z)$ needs to be performed by
different methods depending on the value of $z$ and its position in the complex
plane, and on the required precision.  We follow the book: Sh. E. Tsimring,
<i>Special functions: algorithms and programs</i>
(in Russian), and the paper: 
<i>Algorithm ..., Error function for large real argument</i>,
ACM TOMS ... 197?. (These texts do not consider methods for arbitrary-precision computations.)


The function $Erf(z)$ has the following approximations that are useful for its
numerical computation:

*	1. The Taylor series at $z=0$,
$$ (Sqrt(Pi)/(2*z))*Erf(z) = 1-z^2/3+z^4/(2! *5)- ... + (-z^2)^n/((2*n+1)*n!)+O(z^(2*n+1))$$
*	2. The Taylor series for a slightly modified function,
$$ (Sqrt(Pi)/2)*(e^(z^2)/z)*Erf(z) = 1+(2*z^2)/3+... + (2*z^2)^n/(2*n+1)!! + O(z^(2*n+1))$$.
*	3. Asymptotic expansion of $Erfc(z)$ at $z=Infinity$:
$$ Erfc(z) = e^(-z^2)/(z*Sqrt(Pi))*(1-1/(2*z^2) + ... + (2*n-1)!! /(-2*z^2)^n+...) $$.
*	4. Continued fraction expansion due to Laplace:
$$ (Sqrt(Pi)/2)*z*e^(z^2)*Erfc(z) = z/(z+1/2/(z+1/(z+3/2/(z+...)))) $$
The continued fraction on the RHS of the above equation can be rewritten equivalently as
$$ 1/(1+v/(1+2*v/(1+3*v/(1+...)))) $$
if we define $v:=(2*z^2)^(-1)$.

Here we shall analyze the convergence and precision of these methods and show
which one has to be chosen to compute $Erf(z)$ with (relative) precision $P$
decimal digits for a given (complex) number $z$, and obtain estimates for the
necessary number of terms to take.

Both Taylor series converge absolutely for all $z$, but they do not converge uniformly fast; in fact these series are not very useful for large $z$ because a very large number of slowly decreasing terms gives a significant contribution to the result, and the roundoff error (especially for the first series with the alternating signs) becomes too high. Both series converge well for $Abs(z)<1$.

Consider method 1 (the first Taylor series). We shall use the method 1 only for $Abs(z)<=1$. If the absolute error of the truncated Taylor series is estimated as the first discarded term, the precision after taking all terms up to and including $z^(2*n)$ is approximately $z^(2*n+2)/(n+2)!$. The factorial can be approximated by Stirling's formula,
$ n! <=> n^n * e^(-n)$. The value of $Erf(z)$ at small $z$ is of order 1, so we can take the absolute error to be equal to the relative error of the series that starts with $1$. Therefore, to obtain $P$ decimal digits of precision, we need the number of terms $n$ that satisfies the inequality
$$ Abs(e*z^2/(n+1))^(n+1) < 10^(-P)$$.
(We have substituted $n+1$ instead of $n+2$ which made the inequality stronger.)
The error will be greatest when $Abs(z)=1$. For these values of $z$, the above inequality is satisfied when $n>1+Exp(1+W(P*Ln(10)/e))$ where $W$ is Lambert's $W$ function.

Consider method 3 (the asymptotic series). We shall use the method 3 only for
large enough values of $z$ and small enough precision. The asymptotic series
does not actually converge for any $z$; if we select a large value of $z$, the
terms of the series decrease at first but then start to grow. The way to use an
asymptotic series for a numerical calculation is to truncate the series
<i>well</i> <i>before</i> the terms start to grow. The error of this
approximation is usually not greater than the first discarded term.

The inherent limitation of the method of asymptotic series is that for a given
$z$, there will be a certain place in the series where the term has the minimum
absolute value (after that, the series is unusable), and the error of the
approximation cannot be smaller than that term. This means that for a given
finite $z$, no matter how large, there is a maximum number of digits of
precision that can be achieved with the asymptotic series; getting more
precision requires a different method.

There are two important cases when calculating $Erf(z)$ for large (complex) $z$: the case of $z^2>0$ and the case of $z^2<0$. In the first case (e.g. a real $z$), the function $Erf(z)$ is approximately $1$ for large $Abs(z)$ (if $Re(z)>0$, and approximately $-1$ if $Re(z)<0$). In the second case (e.g. pure imaginary $z=I*t$) the function $Erf(z)$ rapidly grows as $Exp(-z^2)/z$ at large $Abs(z)$.


*REM FIXME need to fill in on the details
