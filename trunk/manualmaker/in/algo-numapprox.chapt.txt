
		Continued fractions

	    Approximation of numbers by continued fractions

*A continued fraction approximation!of rational numbers

The function {ContFrac} converts a (rational) number $r$ into a regular
continued fraction,
$$ r = n[0] + 1/(n[1] + 1/(n[2]+...)) $$.
Here all numbers $n[i]$ ("terms" of a continued fraction) are integers and all except $n[0]$ must be positive.
(Continued fractions may not converge unless their terms are positive and
bounded from below.)

The algorithm for converting a rational number $r=n/m$ into a continued
fraction is simple. First, we determine the integer part of $r$, which is
$Div(n,m)$. If it is negative, we need to subtract one, so that $r=n[0]+x$ and
the remainder $x$ is nonnegative and less than $1$. The remainder
$x=Mod(n,m)/m$ is then inverted, $r[1] := 1/x = m/Mod(n,m)$ and so we have
completed the first step in the decomposition, $r = n[0] + 1/r[1]$; now $n[0]$
is integer but $r[1]$ is perhaps not integer. We repeat the same procedure on
$r[1]$, obtain the next integer term $n[1]$ and the remainder $r[2]$ and so on,
until such $n$ that $r[n]$ is an integer and there is no more work to do. This
process will always terminate because all floating-point values are actually
rationals in disguise.

*A {GuessRational}

Continued fractions are useful in many ways. For example, if we know that a
certain number $x$ is rational but have only a floating-point representation of
$x$ with a limited precision, say, $1.5662650602409638$, we can try to guess its
rational form (in this example $x=130/83$). The function {GuessRational} uses
continued fractions to find a rational number with "optimal" (small) numerator
and denominator that is approximately equal to a given floating-point number.

Consider the following example. The number $17/3$ has a continued fraction
expansion {{5,1,2}}. Evaluated as a floating point number with limited
precision, it may become something like $17/3+0.00001$, where the small number
represents a round-off error. The continued fraction expansion of this number is
{{5, 1, 2, 11110, 1, 5, 1, 3, 2777, 2}}. The presence of an unnaturally large
term $11110$ clearly signifies the place where the floating-point error was
introduced; all terms following it should be discarded to recover the continued fraction {{5,1,2}} and from it the initial number $17/3$.

If a continued fraction for a number $x$ is
cut right before an unusually large term, and
evaluated, the resulting rational number is very close to close to $x$ but has an unusually small
denominator. This works because partial continued fractions provide "optimal"
rational approximations for the final (irrational) number, and because the
magnitude of the terms of the partial fraction is related to the magnitude of
the denominator of the resulting rational approximation.

{GuessRational(x, prec)} needs to choose the place where it should cut the
continued fraction.
The algorithm for this is somewhat heuristic but it works well enough.
The idea is to cut the continued fraction
when adding one more term would change the result by less than the specified
precision. To realize this in practice, we need an estimate of how much a
continued fraction changes when we add one term.

*A continued fraction approximation!error bound

The routine {GuessRational} uses a (somewhat weak) upper bound for the difference of continued fractions that differ only by an additional last term:
$$ Abs(delta) := Abs( 1/(a[1]+1/(...+1/a[n])) - 1/(a[1]+1/(...+1/a[n+1])) ) < 1/ ((a[1]*...*a[n])^2 * a[n+1]) $$.
(The derivation of this inequality is given below.)
Thus we should compute the product of successive terms $a[i]$ of the continued fraction and stop at $a[n]$ at which this product exceeds the maximum number of digits. The routine {GuessRational} has a second parameter {prec} which is by default 1/2 times the number of decimal digits of current precision; it stops at $a[n]$ at which the product $a[1]*...*a[n]$ exceeds $10^prec$.

The above estimate for $delta$ hinges on the inequality
$$ 1/(a+1/(b+...)) < 1/a $$
and is suboptimal if some terms $a[i]=1$, because the product of $a[i]$ does not increase when one of the terms is equal to 1, whereas in fact these terms do make $delta$ smaller. A somewhat better estimate would be obtained if we use the inequality
$$ 1/(a+1/(b+1/(c+...))) < 1/(a+1/(b+1/c)) $$.
(See the next section for more explanations of precision of continued fraction approximations.)
This does not lead to a significant improvement if $a>1$ but makes a difference when $a=1$. In the product $a[1]*...*a[n]$, the terms $a[i]$ which are equal to 1 should be replaced by
$$ a[i]+1/(a[i+1]+1/a[i+2])$$.
Since the comparison of $a[1]*...*a[n]$ with $10^prec$ is qualitative, it it enough to perform calculations of $a[1]*...*a[n]$ with limited precision.

This algorithm works well if $x$ is computed with enough precision; namely, it
must be computed to at least as many digits as there are in the numerator and
the denominator of the fraction combined. Also, the parameter {prec} should not
be too large (or else the algorithm will find another rational number with a larger
denominator that approximates $x$ "better" than the precision to which you know $x$).

*A {NearRational}

The related function {NearRational(x, prec)} works somewhat differently. The
goal is to find an "optimal" rational number, i.e. with smallest numerator and
denominator, that is within the distance $10^(-prec)$ of a given value $x$.
The function {NearRational} does not always give the same answer as {GuessRational}.

The
algorithm for {NearRational} comes from the HAKMEM [Beeler <i>et al.</i> 1972], Item 101C. Their
description is terse but clear:

	Problem: Given an interval, find in it the
	rational number with the smallest numerator and
	denominator.
	Solution: Express the endpoints as continued
	fractions.  Find the first term where they differ
	and add 1 to the lesser term, unless it's last. 
	Discard the terms to the right.  What's left is
	the continued fraction for the "smallest"
	rational in the interval.  (If one fraction
	terminates but matches the other as far as it
	goes, append an infinity and proceed as above.)

The HAKMEM text [Beeler <i>et al.</i> 1972] contains several interesting insights relevant to continued fractions and other numerical algorithms.

	    Accurate computation of continued fractions

Sometimes an analytic function $f(x)$ can be approximated using a continued
fraction that contains $x$ in its terms. Examples include the inverse tangent
$ArcTan(x)$, the error function $Erf(x)$, and the incomplete gamma function
$Gamma(a,x)$ (see below for details). For these functions, continued fractions
provide a method of numerical calculation that works when the Taylor series
converges slowly or not at all. However, continued fractions usually converge
quickly for one value of $x$ but slowly for another. Also, it is not as easy to
obtain an analytic error bound for a continued fraction approximation as it is
for power series.

In this section we describe two methods to compute a continued fraction: the
simple "bottom-up" and the more complicated "top-down" method. The "bottom-up"
method is faster but requires to know the number of terms in advance. The
"top-down" method essentially replaces the continued fraction by a sum of a
certain series.
This method is slower but provides an automatic error estimate and can be used to
evaluate a continued fraction with more and more terms until the desired
precision is achieved.

The formula for the precision of the continued fraction
approximation used in the "top-down" method sometimes allows to estimate the
number of terms in advance. If the number of terms is known, then the faster
"bottom-up" method should be used.

A continued fraction
$$ a[0]+b[0]/(a[1]+b[1]/(a[2]+...))$$
is specified by a set of terms ($a[i]$, $b[i]$).
[If continued fractions are used to approximate analytic functions such as $ArcTan(x)$, then ($a[i]$, $b[i]$) will depend on $x$.]
Let us denote by $F[m][n]$ the truncated fraction containing only the terms from $m$ to $n$,
$$ F[m][n] := a[m]+b[m]/(a[m+1]+b[m+1]/(...+b[n]/a[n]))$$.
In this notation, the continued fraction that we need to compute is $F[0][n]$.
Our task is first, to select a large enough $n$ so that $F[0][n]$ gives enough precision, and second, to compute that value.

*A continued fraction approximation!bottom-up computation

The "bottom-up" method is to select some value of $n$ and start evaluating the fraction from the bottom upwards. First we take $F[n][n]=a[n]$ and then we use the obvious relation of backward recurrence,
$$ F[m][n] = a[m]+b[m]/F[m+1][n] $$,
to obtain successively $F[n-1][n]$, ..., $F[0][n]$. This method requires one long division at each step.

An alternative implementation may be faster in case all $a[i]$, $b[i]$ are
integers.
The idea is to obtain the numerator and the denominator of $F[0][n]$
separately as two simultaneous backward recurrences. If 
$F[m+1][n] = p[m+1]/q[m+1]$, then 
$p[m]=a[m]*p[m+1]+b[m]*q[m+1]$ and $q[m]=p[m+1]$. This method would start with
$p[n]=a[n]$, $q[n]=1$ and require two long multiplications at each step; the only
division will be performed at the very end. Sometimes this method allows to
reduce the round-off error.

*A continued fraction approximation!bottom-up computation!improving precision

There is an improvement to the bottom-up method that can sometimes increase the achieved
precision without computing more terms (this is suggested in [Tsimring
1988], sec. 2.4).
Namely, for the starting value of the recurrence we should choose not
$a[n]$ but another number that more closely approximates the infinite remainder
of the fraction.
The infinite remainder, which can be symbolically written as $F[n][Infinity]$, can be sometimes estimated (obviously, we are unable to compute the remainder exactly -- otherwise we would not embark on computing the continued fraction numerically).
In simple cases, $F[n][Infinity]$ changes very slowly at large $n$
(warning: this is not always true and needs to be verified in each particular case!).
Suppose that $F[n][Infinity]$ is approximately constant; then it must be approximately equal to $F[n+1][Infinity]$.
Therefore, if we solve the (quadratic) equation
$$ x = a[n] + b[n]/x $$,
we shall obtain the (positive) value $x$ which may be a much better
approximation for $F[n][Infinity]$ than $a[n]$. But this depends on the
assumption of the way the continued fraction converges. It may happen,
for example, that for large $n$ the value $F[n][Infinity]$ is almost
the same as $F[n+2][Infinity]$ but is significantly different from
$F[n+1][Infinity]$. Then we should instead solve the (quadratic) equation
$$ x = a[n] + b[n]/(a[n+1]+b[n+1]/x) $$
and take the positive solution $x$ as the approximation for $F[n][Infinity]$.

The "bottom-up" method obviously requires to know the number of terms $n$ in
advance; calculations have to be started all over again if more terms are
needed. Also, it provides no error estimate.

*A continued fraction approximation!top-down computation

The "top-down" method
allows to compute the fraction in the forward direction. The idea
*FOOT This is a known result in the theory of continued fractions. We give an elementary derivation.
is to
replace the continued fraction $F[0][n]$ with a sum of a certain series,
$$ a[0]+b[0]/(a[1]+b[1]/(...+b[n-1]/a[n])) = Sum(k,0,n,f[k]) $$.
Here
$$ f[k]:=F[0][k]-F[0][k-1] $$
($k>=1$) is a sequence that will be calculated in the forward direction, starting from $k=1$.
If we manage to find a formula for this sequence, then adding one
more term $f[k]$ will be equivalent to recalculating the
continued fraction with $k$ terms instead of $k-1$ terms. This will
automatically give an error estimate and allow to compute with more
precision if necessary without having to repeat the calculation from
the beginning. (The transformation of the continued fraction into a
series is exact, not merely an approximation.)

The formula for $f[k]$ is the following.
First the auxiliary sequence $P[k]$, $Q[k]$ for $k>=1$ needs to be
defined by $P[1]=0$, $Q[1]$=1, and $P[k+1]:=b[k]*Q[k]$,
$Q[k+1]:=P[k]+a[k]*Q[k]$.
Then define $f[0] := a[0]$ and 
$$ f[k] := ((-1)^k*b[0]*...*b[k-1])/(Q[k]*Q[k+1]) $$
for $k>=1$.
The "top-down" method consists of computing $f[n]$
sequentially and adding them together, until $n$ is large enough so
that $f[n]/f[0]$ is less than the required precision.
Evaluating the next element $f[k]$ requires four long multiplications
and one division.
This is significantly slower, compared with just one long division in the bottom-up method.
Therefore it is desirable to have an a priori estimate of the convergence rate and to be able to choose the number of terms before the computation.

*A continued fraction approximation!top-down computation!non-alternating signs

Note that typically all terms $a[i]$, $b[i]$ are positive, so the
series we obtained will have terms $f[k]$ of alternating signs. To
reduce the round-off error,
*FOOT This method is used by [Thacher 1963], who refers to a suggestion by Hans Maehly.
we can convert this series into a series
with constant sign by adding together two adjacent elements, say
$f[2*k]+f[2*k+1]$. The relevant formulae can be derived from the
definition of $f[k]$ using the recurrence relations for $P[k]$, $Q[k]$:
$$ f[2*k-1]+f[2*k] = - (b[0]*...*b[2*k-2]*a[2*k]) / (Q[2*k-1]*Q[2*k+1]) $$,
$$ f[2*k]+f[2*k+1] = (b[0]*...*b[2*k-1]*a[2*k+1]) / (Q[2*k]*Q[2*k+2]) $$.
Now in the series $f[0]$+($f[1]+f[2]$)+($f[3]+f[4]$)+... the first term is positive and all subsequent terms will be negative.

	    Derivation of the formula for $f[k]$

*A continued fraction approximation!top-down computation!derivation

Here is a straightforward derivation of the above formula for $f[k]$. We need
to compute the difference between successive approximations $F[0][n]$
and $F[0][n+1]$.
The recurrence relation we shall use is
$$ F[m][n+1] - F[m][n] = -(b[m]*(F[m+1][n+1]-F[m+1][n]))/(F[m+1][n+1]*F[m+1][n]) $$.
This can be shown by manipulating the two fractions, or by using
the recurrence relation for $F[m][n]$.

So far we have reduced the difference between $F[m][n+1]$ and $F[m][n]$
to a similar difference on the next level $m+1$ instead of $m$; i.e. we
can increment $m$ but keep $n$ fixed. We can apply this formula to
$F[0][n+1]-F[0][n]$, i.e. for $m=0$, and continue applying the same
recurrence relation until $m$ reaches $n$. The result
is
$$ F[0][n+1] - F[0][n] = ((-1)^n*b[0]*...*b[n])/(F[1][n+1]*...*F[n+1][n+1]*F[1][n]*...*F[n][n]) $$.

Now the problem is to simplify the two long products in the
denominator. We notice that $F[1][n]$ has $F[2][n]$ in the denominator,
and therefore $F[1][n]*F[2][n]=F[2][n]*a[1]+b[1]$. The next product is
$F[1][n]*F[2][n]*F[3][n]$ and it simplifies to a linear function of
$F[3][n]$, namely $F[1][n]*F[2][n]*F[3][n]$ =
$(b[1]+a[1]*a[2])*F[3][n]+b[1]*a[2]$. So we can see that there is a
general formula
$$  F[1][n]*...*F[k][n] = P[k]+Q[k]*F[k][n] $$
with some coefficients $P[k]$, $Q[k]$ which do not actually depend on
$n$ but only on the terms of the partial fraction up to $k$. In other
words, these coefficients can be computed starting with $P[1]=0$,
$Q[1]=1$ in the forward direction. The recurrence relations for $P[k]$,
$Q[k]$ that we have seen above in the definition of $f[k]$ follow from the identity $(P[k]+Q[k]*F[k][n])*F[k+1][n]$ =
$P[k+1]+Q[k+1]*F[k+1][n]$.

Having found the coefficients $P[k]$, $Q[k]$, we can now rewrite the long products in the denominator, e.g.
$$ F[1][n]*...*F[n][n] = P[n]+Q[n]*F[n][n] = Q[n+1] $$.
(We have used the recurrence relation for $Q[n+1]$.) Now it follows that
$$ f[n+1]:=F[0][n+1]-F[0][n] = ((-1)^n*b[0]*...*b[n])/(Q[n+1]*Q[n+2]) $$.
Thus we have converted the continued fraction into a series, i.e. $F[0][n]=Sum(k,0,n,f[k])$ with $f[k]$ defined above.

	    Estimating convergence of continued fractions

*A continued fraction approximation!convergence rate

Suppose we are given the terms $a[k]$, $b[k]$ that define an infinite continued
fraction, and we are interested to estimate its rate of convergence. 
We need to find the number of terms $n$ for which the error of approximation is
less than a given $epsilon$. In our notation, we need to solve
$$ Abs(f[n+1]) < epsilon $$
for $n$.

The formula that we derived for $f[n+1]$ gives an error estimate for
the continued fraction truncated at the $n$-th term.
But this formula contains the numbers $Q[k]$ in the denominator.
The problem now is to find how quickly the sequence $Q[k]$ grows.

It is not always easy to get a handle on this sequence; in most cases there is
no closed-form expression for $Q[k]$.
The recurrence relation for it can be rewritten as
$$ Q[k+2]=a[k+1]*Q[k+1]+b[k]*Q[k] $$,
for $k>=0$, with initial values $Q[0]=0$ and $Q[1]=1$.

*A continued fraction approximation!error bound

A simple lower bound on the growth can be quickly obtained from this recurrence
relation.
Assume that $a[k]>0$, $b[k]>0$.
It is clear that all $Q[k]$ are positive, and so $Q[k+1]>=a[k]*Q[k]$.
Therefore $Q[k]$ grows at least as the product of all $a[k]$:
$$ Q[k+1] >= Factorize(i,1,k, a[k]) $$.
This result gives the following upper bound on the precision,
$$ Abs(f[n+1]) <= (b[0]*...*b[n])/((a[1]*...*a[n])^2*a[n+1]) $$.

We have used this bound to estimate the relative error of the
continued fraction expansion for $ArcTan(x)$ at small $x$ (elsewhere in this book).
However, we found that at large $x$ this bound becomes greater than 1.
This does not mean that the continued fraction does not converge and cannot be used to compute $ArcTan(x)$ when $x>1$, it merely
indicates that the bound is too weak.
The sequence $Q[k]$ actually grows faster than the product of all $a[k]$ and we
need a tighter bound on this growth.

*A continued fraction approximation!of $Erfc(x)$

We shall now consider another example when the growth of this sequence can be
estimated analytically. 
The complementary error function $Erfc(x)$ can be computed using the
continued fraction due to Laplace,
$$ Sqrt(Pi)/2*x*Exp(x^2)*Erfc(x)=1/(1+v/(1+(2*v)/(1+(3*v)/(1+...)))) $$,
where $v:=(2*x^2)^(-1)$ is a small parameter, $v<1/2$. 
The terms of this continued fraction are: $a[k]=1$, $b[k]=k*v$, for $k>=1$, and $a[0]=0$, $b[0]=1$.

The simple bound would give $Abs(f[n+1])<=(v^n*n!)$ and this expression grows with $n$.
But we know that the above continued fraction actually converges for any $v$, so $f[n+1]$ must tend to zero for large $n$.
It seems that the simple bound is not strong enough for any $v$ and we need a better bound.

*A method of steepest descent

We can use the formula for $f[k]$ as a bound if we know how quickly the sequence $Q[k]$ grows.
The asymptotic growth of the sequence $Q[k]$ can be estimated in this case by the method of steepest descent, also known as Laplace's method.
(See, e.g., [Olver 1974],
ch. 3, sec. 7.5.)
This method is somewhat complicated but quite powerful.
The method requires that we find an integral representation for $Q[k]$, and then we can obtain the dominant contribution to the integral as an asymptotic series in $k^(-1)$.
We actually only need the leading term of the series which is easy to find (even without a CAS).

*A generating function of a sequence

An integral representation for $Q[k]$ can be obtained using the method of generating functions.
Consider a function $G(s)$ defined by the infinite series
$$ G(s) = Sum(k, 0, Infinity, Q[k+1]*(I*s)^k/k!) $$.
$G(s)$ is usually called the "generating function" of a sequence.
We shifted the index to $k+1$ for cosmetic convenience, since $Q[0]=0$.
The imaginary unit in front of $s$ is not strictly necessary to introduce but
it usually makes the generating function nicer (on the real axis, that is).

*A generating function of a sequence!obtaining

It is generally the case that if we know a simple linear recurrence relation for a sequence, then we can also easily find its generating function.
The generating function will satisfy a linear differential equation.
To guess this equation, we write down the series for $G(s)$ and its
derivative $D(s)G(s)$ and try to find their linear combination which is
identically zero because of the recurrence relation.
(There is, of course, a computer algebra algorithm for doing this automatically.)

In the case of our sequence $Q[k]$ above, we find that
$$ I*(D(s)G(s)) = (1-I*v*s)*G(s) $$.
The solution with the obvious initial condition $G(0)=1$ is
$$ G(s) = Exp(-I*s-v*s^2/2) $$.

*A generating function of a sequence!integral representation

The second step is to obtain an integral representation for $Q[k]$.
This is possible because $Q[k+1]$ is equal to the $k$-th derivative of the generating function at $s=0$:
$$ Q[k+1] = I^k*D(s, k) G(s=0) $$.
So if we had an integral representation for $G(s)$, we could also obtain one for $Q[k]$ by differentiation.

Now we use the fact that derivatives of Fourier transforms are "easy".
The generating function $G(s)$ has the following Fourier representation,
$$ G(s) = (1/Sqrt(2*Pi*v)) * Integrate(t, -Infinity, Infinity) Exp(-t^2/(2*v)-I*s*(t+1)) $$.
Taking the derivatives with respect to $s$ at $s=0$, we obtain an integral representation for $Q[k]$:
$$ Q[n+1] = (1/Sqrt(2*Pi*v)) * Integrate(t, -Infinity, Infinity) (1+t)^n * Exp(-t^2/(2*v)) $$.

*A method of steepest descent!example

Now we can use the method of steepest descent.
We represent the integrand as an exponential of some function $g(t,n)$ and find "stationary points" where this function has local maxima:
$$ Q[n+1] = (1/Sqrt(2*Pi*v)) * Integrate(t, -Infinity, Infinity) Exp(g(t,n)) $$,
$$ g(t,n) := -t^2/(2*v)+n*Ln(1+t) $$.
(Note that the logarithm here must acquire an imaginary part $I*Pi$ for $t<-1$,
but we shall see that the integral over negative $t$ is negligible.)
We expect that when $n$ is large, $g(t,n)$ will have a peak or several peaks at certain values of $t$.
At $t$ away from the peaks, the value of $g(t,n)$ is much smaller and,
since $g$ is in the exponential, the integral is dominated by the
contribution of the peaks.
This is the essence of the method of steepest descent.

We find that in our case, two peaks occur at approximately $t1<=> -1/2+Sqrt(n*v)$ and $t2<=> -1/2-Sqrt(n*v)$. 
We assume that $n$ is large enough so that $n*v>1/2$. Then the first peak is at a positive $t$ and the second peak is at a negative $t$.

The contribution of the peaks can be computed from the Taylor approximation of $g(t,n)$ near the peaks.
We can expand, for example,
$$ g(t,n) <=> g(t1,n) + (Deriv(t,2)g(t1,n))*(t-t1)^2/2 $$
near $t=t1$.
The values $g(t1,n)$ and $Deriv(t,2)g(t1,n)$, and likewise for $t2$, are constants that we already know since we know $t1$ and $t2$.
Then the integral of $Exp(g)$ will be approximated by the integral
$$ Integrate(t, -Infinity, Infinity) Exp(g(t1,n) + (Deriv(t,2)g(t1,n))*(t-t1)^2/2) $$.
(Note that $Deriv(t,2)g(t1,n)$ is negative.)
This is a Gaussian integral that can be easily evaluated, with the result
$$ exp(g(t1,n))*Sqrt(-(2*Pi)/Deriv(t,2)g(t1,n)) $$.
This is the leading term of the contribution of the peak at $t1$;
there will be a similar expression for the contribution of $t2$.
We find that the peak at $t1$ gives a larger contribution, by the factor $Exp(2*Sqrt(n/v))$.
This factor is never small since $n>1$ and $v<1/2$.
So it is safe to ignore the peak at $t2$ for the purposes of our error analysis.

*A Stirling's formula

*A continued fraction approximation!of $Erfc(x)$!error estimate

We find, after some algebra, that the leading asymptotic of $Q[k]$ is
$$ Q[n+1] <=> 1/Sqrt(2)*Exp(Sqrt(n/v)-1/(4*v)-n/2)*(v*n)^(n/2) $$.
This, together with Stirling's formula
$$ n! <=> Sqrt(2*Pi*n)*(n/e)^n $$,
allows to estimate the error of the continued fraction approximation:
$$ Abs(f[n+1]) <=> 2*Sqrt((2*Pi*n)/v)*Exp(-n/2-2*Sqrt(n/v)+1/(2*v)) $$.

Note that this is not merely a bound but an actual asymptotic estimate of $Abs(f[n+1])$.
(By the way, Stirling's formula can be also derived using the method of
steepest descent from an integral representation of the Gamma function,
in a similar way.)


We find that the error of the continued fraction approximation to $Erfc(x)$
decreases with $n$ in a geometric progression
(i.e. roughly as some constant to the power of $n$), for any $v$.

	   Usefulness of continued fractions

Many mathematical functions have a representation as a continued fraction.
Some systems of "exact arithmetic" even use continued fractions as a primary internal representation of real numbers.
This has its advantages (no round-off errors, lazy "exact" computations) and disadvantages (it is slow, especially with some operations).
Here we consider the application of continued fractions to a traditional implementation of arithmetic (floating-point numbers with variable precision).

Usually, a continued fraction representation to a function will converge geometrically, i.e. $O(P)$ terms are needed for a precision of $P$ digits.
If a geometrically convergent Taylor series representation is also available, the continued fraction method will always be slower because it requires at least as many or more long multiplications per term, and because the Taylor series in most cases can be summed efficiently using the rectangular scheme.

However, there are some functions for which a Taylor series is not easily computable but a simple continued fraction is available.
For example, incomplete Gamma functions (in particular the error function $Erf(x)$) can be computed using the continued fractions in some domains of their arguments.

		Newton's method and its improvements

The Newton-Raphson method of numerical solution of algebraic equations and its generalizations
can be used to obtain multiple-precision values of several elementary functions.

	    Newton's method

*A Newton's method

The basic formula is widely known: If $f(x)=0$ must be solved, one starts with a value of $x$ that is close to some root and iterates $$x'=x-f(x)*(D(x)f(x))^(-1)$$.
This formula is based on the approximation of the function $f(x)$ by a tangent line at some point $x$. A Taylor expansion in the neighborhood of the root shows that (for an initial value $x[0]$ sufficiently close to the root) each iteration gives at least twice as many correct digits of the root as the previous one ("quadratic convergence"). Therefore the complexity of this algorithm is proportional to a logarithm of the required precision and to the time it takes to evaluate the function and its derivative. Generalizations of this method require computation of higher derivatives of the function $f(x)$ but successive approximations to the root converge several times faster (the complexity is still logarithmic).

*A Newton's method!initial value

Newton's method sometimes suffers from a sensitivity to the initial guess.
If the initial value $x[0]$ is not chosen sufficiently close to the root, the iterations may converge very slowly or not converge at all. To remedy this, one can combine Newton's iteration with simple bisection. Once the root is bracketed inside an interval ($a$, $b$), one checks whether $(a+b)/2$ is a better approximation for the root than that obtained from Newton's iteration. This guarantees at least linear convergence in the worst case.

*A Newton's method!cubic convergence

For some equations $f(x)=0$, Newton's method converges faster; for example, solving $Sin(x)=0$ in the neighborhood of $x=3.14159$ gives "cubic" convergence, i.e. the number of correct digits is tripled at each step. This happens because $Sin(x)$ near its root $x=Pi$ has vanishing second derivative and thus the function is particularly well approximated by a straight line.

	    Halley's method

*A Halley's method

<i>Halley's method</i> is an improvement to Newton's method that makes each equation well approximated by a straight line near the root. Edmund Halley computed fractional powers, $x=a^(1/n)$, by the iteration
$$ x'=x*(n*(a+x^n)+(a-x^n))/(n*(a+x^n)-(a-x^n)) $$.
This formula is equivalent to Newton's method applied to the equation
$x^(n-q) = a*x^(-q)$ with $q=(n-1)/2$. This iteration has a cubic convergence rate. This is the fastest method to compute $n$-th roots ($n>=3$) with multiple precision. Iterations with higher order of convergence, for example, the method with quintic convergence rate
$$ x' = x* ( (n-1)/(n+1)*(2*n-1)/(2*n+1) *x^(2*n) + 2*(2*n-1)/(n+1)*x^n*a + a^2) / (x^(2*n)+2*(2*n-1)/(n+1)*x^n*a+(n-1)/(n+1)*(2*n-1)/(2*n+1)*a^2) $$,
require more arithmetic operations per step and are in fact less efficient at high precision.

Halley's method can be generalized to any function $f(x)$. A cubically convergent iteration is always obtained if we replace the equation $f(x)=0$ by an equivalent equation
$$ g(x):=f(x)/Sqrt(Abs(D(x)f(x))) = 0 $$
and use the standard Newton's method on it.
Here the function $g(x)$ is chosen so that its second derivative vanishes ($D(x,2)g(x)=0$) at the root of the equation $f(x)=0$, independently of where this root is.
(There is no unique choice of the function $g(x)$ and sometimes another choice will make the iteration more quickly computable.)

*A Halley's method!explicit formula

The Halley iteration for the equation $f(x)=0$ can be written as
$$ x'=x-(2*f(x)*D(x)f(x))/(2*(D(x)f(x))^2-f(x)*Deriv(x,2)f(x)) $$.
For example, the equation $Exp(x)=a$ is transformed into $g(x) := Exp(x/2)-a*Exp(-x/2)=0$.

Halley's iteration, despite its faster convergence rate, may be more cumbersome to evaluate than Newton's iteration and so it may not provide a more efficient numerical method for some functions. Only in some special cases is Halley's iteration just as simple to compute as Newton's iteration. But Halley's method has another advantage: it is generally less sensitive to the choice of the initial point $x[0]$. An extreme example of sensitivity to the initial point is the equation $x^(-2)= 12$ for which Newton's iteration $x'=3*x/2-6*x^3$ converges to the root only from initial points $0<x[0] <0.5$ and wildly diverges otherwise, while Halley's iteration converges to the root from any $x[0]>0$.

It is at any rate not true that Halley's method always converges better than Newton's method. For instance, it diverges on the equation $2*Cos(x)=x$ unless started at $x[0]$ within the interval ($-1/6*Pi$,$7/6*Pi$). Another example is the equation $Ln(x)=a$. This equation allows to compute $x=Exp(a)$ if a fast method for computing $Ln(x)$ is available (e.g. the AGM-based method). For this equation, Newton's iteration
$$ x' = x*(1+a-Ln(x)) $$
converges for any $0<x<Exp(a+1)$, while Halley's iteration
converges only if $Exp(a-2)<x<Exp(a+2)$.

*A Halley's method!when to use

When it converges, Halley's iteration can still converge very slowly for certain functions $f(x)$, for example, for $f(x)=x^n-a$ if $n^n>a$. For such functions that have very large and rapidly changing derivatives, no general method can converge faster than linearly. In other words, a simple bisection will generally do just as well as any sophisticated iteration, until the root is approximated relatively precisely. Halley's iteration combined with bisection seems to be a good choice for such problems.

However, despite its faster convergence, Halley's iteration frequently gives no advantage over Newton's method.
To obtain $P$ digits of the result, we need about $Ln(P)/Ln(2)$ iterations of a quadratically convergent method and about $Ln(P)/Ln(3)$ iterations of a cubically convergent method.
So the cubically convergent iteration is faster only if the time taken by one iteration is less than about $Ln(3)/Ln(2)<=>1.6$ of the time of one quadratic iteration.

	    Higher-order schemes

*A Newton's method!higher-order schemes

In some cases it is easy to generalize Newton's iteration to higher-order schemes.
There are general formulae such as Shroeder's and Householder's iterations.
We shall give some examples where the construction is very straightforward.
In all examples $x$ is the initial approximation and the next approximation is obtained by truncating the given series.

*	1. Inverse $1/a$.
Set $y=1-a*x$, then
$$ 1/a = x/(1-y) = x*(1+y+y^2+...) $$.
*	1. Square root $Sqrt(a)$.
Set $y=1-a*x^2$, then
$$ Sqrt(a) = Sqrt(1-y)/x = 1/x*(1-1/2*y-1/8*y^2-...) $$.
*	1. Inverse square root $1/Sqrt(a)$.
Set $y=1-a*x^2$, then
$$ 1/Sqrt(a) = x/Sqrt(1-y) = x*(1+1/2*y+3/8*y^2+...) $$.
*	1. $n$-th root $a^(1/n)$.
Set $y=1-a*x^n$, then
$$ a^(1/n) = (1-y)^(1/n)/x = 1/x*(1-1/n*y-(n-1)/(2*n^2)*y^2-...) $$.
*	1. Exponential $Exp(a)$.
Set $y=a-Ln(x)$, then
$$ Exp(a) = x*Exp(y) = x*(1+y+y^2/2! + y^3/3! +...) $$.
*	1. Logarithm $Ln(a)$.
Set $y=1-a*Exp(-x)$, then
$$ Ln(a) = x+Ln(1-y) = x-y-y^2/2-y^3/3-... $$.


In the above examples, $y$ is a small quantity and the series represents corrections to the initial value $x$, therefore the order of convergence is equal to the first discarded order of $y$ in the series.

These simple constructions are possible because the functions satisfy simple identities, such as $Exp(a+b)=Exp(a)*Exp(b)$ or $Sqrt(a*b)=Sqrt(a)*Sqrt(b)$.
For other functions the formulae quickly become very complicated and unsuitable for practical computations.

	    Precision control

*A Newton's method!precision control


Newton's method is particularly convenient for multiple precision calculations because of its insensitivity to accumulated errors: if $x[k]$ at some iteration is found with a small error, the error will be corrected at the next iteration. Therefore it is not necessary to compute all iterations with the full required precision; each iteration needs to be performed at the precision of the root expected from that iteration. For example, if we know that the initial approximation is accurate to 3 digits, then (assuming quadratic convergence)
*FOOT This disregards the possibility that the convergence might be slightly slower. For example, when the precision at one iteration is $n$ digits, it might be $2*n-10$ digits at the next iteration. In these (fringe) cases, the initial approximation must be already precise enough (e.g. to at least 10 digits in this example).
it is enough to perform the first iteration to 6 digits, the second iteration to 12 digits and so on. In this way, multiple precision calculations are enormously speeded up.

For practical evaluation, iterations must be supplemented with "quality control".
For example, if $x0$ and $x1$ are two consecutive approximations that are
already very close, we can quickly compute the achieved (relative) precision by
finding the number of leading zeros in the number $Abs(x0-x1)/Max(x0,x1)$. This
is easily done using the bit count function. After performing a small number of
initial iterations at low precision, we can make sure that $x1$ has at least a
certain number of correct digits of the root. Then we know which precision to
use for the next iteration (e.g. triple precision if we are using a cubically
convergent scheme). It is important to perform each iteration at the precision
of the root which it will give and not at a higher precision; this saves a
great deal of time since all calculations are very slow at high precision.

	    Fine-tuning the working precision

To reduce the computation time, it is important to write the iteration formula with explicit separation of higher-order quantities.
For example, Newton's iteration for the inverse square root $1/Sqrt(a)$ can be written either as
$$ x' = x*(3-a*x^2)/2 $$
or equivalently as
$$ x' = x + x*(1-a*x^2)/2 $$.
At first sight the first formula seems simpler because it saves one long addition.
However, the second formula can be computed significantly faster than the first one, if we are willing to exercise a somewhat more fine-grained control of the working precision.

Suppose $x$ is an approximation that is correct to $P$ digits; then we expect the quantity $x'$ to be correct to $2*P$ digits.
Therefore we should perform calculations in the first formula with $2*P$ digits;
this means three long multiplications, $3*M(2*P)$.
Now consider the calculation in the second formula.
First, the quantity $y:=1-a*x^2$ is computed using two $2*P$-digit multiplications.
*FOOT In fact, both multiplications are a little shorter, since $x$ is a number with only $P$ correct digits; we can compute $a*x$ and then $a*x^2$ as products of a $2*P$-digit number and a $P$-digit number, with a $2*P$-digit result. We ignore this small difference.
Now, the number $y$ is small and has only $P$ nonzero digits.
Therefore the third multiplication $x*y$ costs only $M(P)$, not $M(2*P)$.
This is a significant time savings, especially with slower multiplication.
The total cost is now $2*M(2*P)+M(P)$.

The advantage is even greater with higher-order methods.
For example, a fourth-order iteration for the inverse square root can be written as
$$ x' = x + 1/2*x*y + 3/8*x*y^2 + 5/16*x*y^3 $$,
where $y := 1-a*x^2$.
Suppose $x$ is an approximation that is correct to $P$ digits; we expect $4*P$ correct digits in $x'$.
We need two long multiplications in precision $4*P$ to compute $y$, then $M(3*P)$ to compute $x*y$, $M(2*P)$ to compute $x*y^2$, and $M(P)$ to compute $x*y^3$.
The total cost is $2*M(4*P)+M(3*P)+M(2*P)+M(P)$.

*A Newton's method!asymptotic cost

The asymptotic cost of finding the root $x$ of the equation $f(x)=0$ with $P$ digits of precision is usually the same as the cost of computing $f(x)$ [Brent 1975].
The main argument can be summarized by the following simple example.
To get the result to $P$ digits, we need $O(Ln(P))$ Newton's iterations.
At each iteration we shall have to compute the function $f(x)$ to a certain number of digits.
Suppose that we start with one correct digit and that each iteration costs us $c*M(2*P)$ operations where $c$ is a given constant, while the number of correct digits grows from $P$ to $2*P$.
Then the total cost of $k$ iterations is
$$c*M(2)+c*M(4)+c*M(8)+...+c*M(2^k)$$.
If the function $M(P)$ grows linearly with $P=2^k$, then we can estimate this sum roughly as $2*c*M(P)$; if $M(P)=O(P^2)$ then the result is about $4/3*c*M(P)$.
It is easy to see that when $M(P)$ is some power of $P$ that grows faster than linear, the sum is not larger than a small multiple of $M(P)$.


Thus, if we have a fast method of computing, say, $ArcTan(x)$, then we immediately obtain a method of computing $Tan(x)$ which is asymptotically as fast (up to a constant).


	    Choosing the optimal order

*A Newton's method!optimal order

Suppose we need to compute a function by Newton's method to precision $P$.
We can usually find iterations of any order of convergence.
For example, a $k$-th order iteration for the reciprocal $1/a$ is
$$ x' = x+x*y+x*y^2+...+x*y^(k-1) $$,
where $y:=1-a*x$.
The cost of one iteration with final precision $P$ is
$$ C(k,P) := M(P/k)+M((2*P)/k)+M((3*P)/k)+...+c*M(P) $$.
(Here the constant $c:=1$ is introduced for later convenience.
It denotes the number of multiplications needed to compute $y$.)

Increasing the order by $1$ costs us comparatively little, and
we may change the order $k$ at any time.
Is there a particular order $k$ that gives the smallest computational cost and should be used for all iterations, or the order needs to be adjusted during the computation?
A natural question is to find the optimal computational strategy.

It is difficult to fully analyze this question, but seems that choosing a particular order $k$ for all iterations is close to the optimal strategy.

A general "strategy" is a set of orders $S(P,P[0])$=($k[1]$, $k[2]$, ..., $k[n]$) to be chosen at the first, second, ..., $n$-th iteration, given the initial precision $P[0]$ and the required final precision $P$.
At each iteration, the precision will be multiplied by the factor $k[i]$.
The optimal strategy $S(P,P[0])$ is a certain function of $P[0]$ and $P$ such that the required precision is reached, i.e.
$$ P[0]*k[1]*...*k[n] = P $$,
and the cost
$$ C(k[1],P[0]*k[1])+C(k[2],P[0]*k[1]*k[2])+ ...+C(k[n],P) $$
is minimized.

If we assume that the cost of multiplication $M(P)$ is proportional to some power of $P$, for instance $M(P)=P^mu$, then
the cost of each iteration and the total cost are homogeneous functions of $P$ and $P[0]$.
Therefore the optimal strategy is a function only of the ratio $P/P[0]$.
We can multiply both $P[0]$ and $P$ by a constant factor and the optimal strategy will remain the same.
We can denote the optimal strategy $S(P/P[0])$.

We can check whether it is better to use several iterations at smaller orders instead of one iteration at a large order.
Suppose that  $M(P)=P^mu$, the initial precision is 1 digit, and the final precision $P=k^n$.
We can use either $n$ iterations of the order $k$ or 1 iteration of the order $P$.
The cost of one iteration of order $P$ at target precision $P$ is
$ C(P,P)$, whereas the total cost of $n$ iterations of order $k$ is
$$ C(k,k)+C(k,k^2)+...+C(k,k^n) $$.
We can take approximately
$$ C(k,p) <=> p^mu*(c-1+k/(mu+1)) $$.
Then the cost of one $P$-th order iteration is
$$ P^mu*(c-1+P/(mu+1)) $$,
while the cost of $n$ iterations of the order $k$ is clearly smaller since $k<P$,
$$ P^mu*(c-1+k/(mu+1))*k^mu/(k^mu-1) $$.
At fixed $P$, the best value of $k$ is found by minimizing this function.
For $c=1$ (division) we find $k=(1+mu)^(1/mu)$ which is never above $2$.
This suggests that $k=2$ is the best value for the division.
However, larger values of $c$ can give larger values of $k$.
The  equation for the optimal value of $k$ is
$$ k^(mu+1)/(mu+1)-k=mu*(c-1) $$.

But we have only considered strategies that use the same order $k$ for all iterations, and we have not yet shown that such strategies are the best ones.
We give a plausible argument (i.e. not quite a rigorous proof) to justify this claim.

Consider the optimal strategy $S(P^2)$ for the initial precision $1$ and the final precision $P^2$, when $P$ is very large.
Since it is better to use several iterations at lower orders, we may assume that the strategy $S(P^2)$ contains many iterations and that one of these iterations reaches precision $P$.
Then the strategy $S(P^2)$ is equivalent to a sequence of the two optimal strategies to go from $1$ to $P$ and from $P$ to $P^2$.
However, both strategies must be the same $S(P)$ since the optimal strategy only depends on the ratio of precisions.
Therefore, the optimal strategy $S(P^2)$ is a sequence of two identical strategies ($S(P)$, $S(P)$).

Suppose that $k[1]$ is the first element of $S(P)$.
The optimal strategy to go from precision $k[1]$ to precision $P*k[1]$ is also $S(P)$.
Therefore the second element of $S(P)$ is also equal to $k[1]$, and by extension
all elements of $S(P)$ are the same.

A similar consideration gives the optimal strategy for other iterations that compute inverses of analytic functions, such as Newton's iteration for the inverse square root or for higher roots.
The difference is that the value of $c$ should be chosen as the equivalent number of multiplications needed to compute the function.
For instance, $c=1$ for division and $c=2$ for the inverse square root iteration.

The conclusion is that in each case we should compute the optimal order $k$ in advance and use this order for all iterations.




		Fast evaluation of Taylor series

*A Taylor series

Taylor series for analytic functions  is a common method of evaluation.

Take, for example, the series for $Exp(x)$.
To straightforwardly evaluate
$$Exp(x)<=>Sum(k,0,N-1,x^k/k!)$$
with $P$ decimal digits of precision and $x<2$, one would need about $N<=>P*Ln(10)/Ln(P)$ terms of the series.
To evaluate the truncated series term by term, one needs $N-1$ long multiplications.
(Divisions by large integers $k!$ can be replaced by a short division of the previous term by $k$. Also, we do not need to evaluate powers $x^k$ separately, since the next term is obtained from the previous one by multiplication by $x$.) In addition, about $Ln(N)/Ln(10)$ decimal digits will be lost due to accumulated round-off errors; therefore the working precision must be increased by this many digits.

If we do not know in advance how many terms of the Taylor series we need, we cannot do any better than just evaluate each term and check if it is already small enough. So in this case we will have to perform $O(N)$ long multiplications.
However, we can organize the calculation much more efficiently if we are able to estimate the necessary number of terms and to afford some storage.

There are two cases: first, the argument $x$ is a small integer or rational number with very few digits; second, the argument $x$ has $P$ digits.
In the first case, it is better to use either Horner's scheme (for small $P$) or the binary splitting technique (for large $P$).
In the second case, it is better to use the "rectangular" algorithm.
In both cases we need to know the number of terms in advance, as we will have to repeat the whole calculation if a few more terms are needed.

*A Taylor series!by Horner's scheme
*A Horner's scheme

Horner's scheme is widely known and consists of the following rearrangement,
$$ Sum(k,0,N-1,a[k]*x^k) = a[0]+x*(a[1]+x*(a[2]+x*( ... + x*a[N-1]))) $$
The calculation is started from the last coefficient $a[N-1]$ toward the first.
If $x$ is small, then the round-off error accumulated during the summation is constantly being multiplied by a small number $x$ and thus is significantly reduced.
If the coefficients $a[k]$ are related by a simple ratio, then the scheme may be modified to simplify the calculations.
A precision control may be also applied to reduce the computation time: for example, $x*a[N-1]$ needs to be computed with very low precision if $x$ is small.
This does not change the asymptotic complexity of the method: it still requires $O(N)=O(P)$ multiplications by $x$, so for general real $x$ the complexity is $O(P*M(P))$.
However, if $x$ is a small rational number, then the multiplication by $x$ is short and takes $O(P)$ operations.
In that case, the total complexity of the method is $O(P^2)$.


*A Taylor series!by $O(Sqrt(N))$ "rectangular" method

The "rectangular" algorithm uses $2*Sqrt(N)$ long multiplications (assuming that the coefficients of the series are short rational numbers) and $Sqrt(N)$ units of storage. (See [Smith 1985].)
For high-precision $x$, this method provides a significant advantage over Horner's scheme.

Suppose we need to evaluate $Sum(k,0,N, a[k]*x^k)$ and we know the number of terms $N$ in advance.
Suppose also that the coefficients $a[k]$ are rational numbers with small numerators and denominators, so a  multiplication $a[k]*x$ is not a long multiplication (usually, either $a[k]$ or the ratio $a[k]$/$a[k-1]$ is a short rational number). Then we can organize the calculation in a rectangular array with $c$ columns and $r$ rows like this,
$$ a[0]+a[r]*x^r+...+a[(c-1)*r]*x^((c-1)*r) $$+
$$ x*(a[1]+a[r+1]*x^r+...+a[(c-1)*r+1]*x^((c-1)*r)) $$+
$$ ... $$+
$$ x^(r-1)*(a[r-1]+a[2*r+1]*x^r+...) $$.
To evaluate this rectangle, we first compute $x^r$ (which, if done by the fast
binary algorithm, requires $O(Ln(r))$ long multiplications). Then we compute
the $c-1$ successive powers of $x^r$, namely
$x^(2*r)$, $x^(3*r)$, ..., $x^((c-1)*r)$ in $c-1$ long
multiplications. The partial sums in the $r$ rows are evaluated column by column as more powers of $x^r$ become available. This requires storage of $r$ intermediate results but no more long multiplications by $x$. If a simple formula relating the coefficients $a[k]$ and $a[k-1]$ is available, then a whole column can be computed and added to the accumulated row values using only short operations, e.g. $a[r+1]*x^r$ can be computed from $a[r]*x^r$ (note that each column contains some consecutive terms of the series). Otherwise, we would need to multiply each coefficient $a[k]$ separately by the power of $x$; if the coefficients $a[k]$  are short numbers, this is also a short operation. After this, we need $r-1$ more
multiplications for the vertical summation of rows (using the Horner scheme). We have potentially saved time because we do not
need to evaluate powers such as $x^(r+1)$ separately, so we do not have to multiply $x$ by itself quite so many
times.

The total required number of long multiplications is $r+c+Ln(r)-2$. The minimum number of multiplications, given that $r*c>=N$, is around  $2*Sqrt(N)$ at $r<=>Sqrt(N)-1/2$.
Therefore, by arranging the Taylor series in a rectangle with sides $r$ and $c$,
we obtain an algorithm which costs $O(Sqrt(N))$ instead of $O(N)$ long multiplications and requires $Sqrt(N)$ units of storage.

One might wonder if we should not try to arrange the Taylor series in a cube or another multidimensional matrix instead of a rectangle. However, calculations show that this does not save time: the optimal arrangement is the two-dimensional rectangle.

The rectangular method saves the number of long multiplications by $x$ but increases the number of short multiplications and additions.
If $x$ is a small integer or a small rational number, multiplications by $x$ are fast and it does not make sense to use the rectangular method.
Direct evaluation of the series is more efficient in that case.
When the coefficients of the series are also small rationals, and when a fast multiplication is available (faster than quadratic), then the binary splitting method (see below) is the best one to use.
With all rational arguments and slow multiplication, Horner's scheme seems to be the best choice.

	    Truncating the working precision

At the $k$-th step of the rectangular method, we are evaluating the $k$-th column with terms containing $x^(r*k)$.
Since a power series in $x$ is normally used at small $x$, the number $x^(r*k)$ is typically much smaller than $1$.
This number is to be multiplied by some $a[i]$ and added to the previously computed part of each row, which is not small.
Therefore we do not need all $P$ floating-point digits of the number $x^(r*k)$,
and the precision with which we obtain it can be gradually decreased from column to column.
For example, if $x^r < 10^(-M)$, then we only need $P-k*M$ decimal digits of $x^(r*k)$ when evaluating the $k$-th column.
(This assumes that the coefficients $a[i]$ do not grow, which is the case for most of the practically useful series.)

Reducing the working precision saves some computation time.
(We also need to estimate $M$ but this can usually be done quickly by bit counting.)
Instead of $O(Sqrt(P))$ long multiplications at precision $P$, we now need one long multiplication at precision $P$, another long multiplication at precision $P-M$, and so on.
This technique will not change the asymptotic complexity which remains $O(Sqrt(P)*M(P))$, but it will reduce the constant factor in front of the $O$.


	    Speed-up for some functions

*A Taylor series!$O(N^(1/3))$ method

An additional speed-up is possible if the function allows a transformation that reduces $x$ and makes the Taylor series converge faster.
For example, $Ln(x)=2*Ln(Sqrt(x))$, $Cos(2*x)=2*Cos(x)^2-1$ (bisection), and $Sin(3*x)=3*Sin(x)-4*Sin(x)^3$ (trisection) are such transformations.
It may be worthwhile to perform a number of such transformations before evaluating the Taylor series, if the time saved by its quicker convergence is more than the time needed to perform the transformations.
The optimal number of transformations can be estimated.
Using this technique in principle reduces the cost of Taylor series from $O(Sqrt(N))$ to $O(N^(1/3))$ long multiplications. However, additional round-off error may be introduced by this procedure for some $x$.

For example, consider the Taylor series for $Sin(x)$,
$$ Sin(x) <=>Sum(k,0,N-1,(-1)^k*(x)^(2*k+1)/(2*k+1)!) $$.
It is sufficient to be able to evaluate $Sin(x)$ for $0<x<Pi/2$.
Suppose we perform $l$ steps of the trisection and then use the Taylor series with the rectangular method.
Each step of the trisection needs two long multiplications.
The value of $x$ after $l$ trisection steps becomes much smaller, $x'=x*3^(-l)$.
For this $x'$, the required number of terms in the Taylor series for $P$ decimal digits of precision is
$$ N <=> (P*Ln(10))/(2*(Ln(P)-Ln(x')))-1 $$.
The number of long multiplications in the rectangular method is $2*Sqrt(N)$.
The total number of long multiplications, as a function of $l$, has its minimum at
$$ l <=> (32*Ln(10)/Ln(3)*P)^(1/3)-(Ln(P)-Ln(x))/Ln(3) $$,
where it has a value roughly proportional to $P^(1/3)$.
Therefore we shall minimize the total number of long multiplications if we first perform $l$ steps of trisection and then use the rectangular method to compute  $N$ terms of the Taylor series.

		Using asymptotic series for calculations

*A asymptotic series

Several important analytic functions have asymptotic series expansions.
For example, the complementary error function $Erfc(x)$ and Euler's Gamma function $Gamma(x)$ have the following asymptotic expansions at large (positive) $x$:

$$ Erfc(x) = e^(-x^2)/(x*Sqrt(Pi))*(1-1/(2*x^2) + ... + (2*n-1)!! /(-2*x^2)^n+...) $$,

$$ Ln(Gamma(x)) = (x-1/2)*Ln(x)-x+Ln(2*Pi)/2 $$
$$ + Sum(n,1,Infinity, B[2*n]/(2*n*(2*n-1)*x^(2*n-1))) $$
(here $B[k]$ are Bernoulli numbers).

The above series expansions are asymptotic in the following sense:
if we truncate the series and then take the limit of very large $x$,
then the difference between the two sides of the equation goes to zero.

It is important that the series be first truncated and then the limit of large $x$ be taken.
Usually, an asymptotic series, if taken as an infinite series,
does not actually converge for any finite $x$.
This can be seen in the examples above.
For instance, in the asymptotic series for $Erfc(x)$ the $n$-th term has $(2*n-1)!!$ in the numerator which grows faster than the $n$-th power of any number.
The terms of the series decrease at first but then eventually start to grow,
even if we select a large value of $x$.

The way to use an
asymptotic series for a numerical calculation is to truncate the series
<i>well before</i> the terms start to grow.

Error estimates of the asymptotic series are sometimes difficult, but the rule of thumb seems to be that the error of the
approximation is usually not greater than the first discarded term of the series.
This can be understood intuitively as follows.
Suppose we truncate the asymptotic series at a point where the terms still decrease, safely before they start to grow.
For example, let the terms around the 100-th term be $A[100]$, $A[101]$, $A[102]$, ...,
each of these numbers being significantly smaller than the previous one,
and suppose we retain $A[100]$ but drop the terms after it.
Then clearly our approximation would have been a lot better if we retained $A[101]$ as well.
Therefore the error of the approximation is approximately equal to $A[101]$.


The inherent limitation of the method of asymptotic series is that for a given
$x$, there will be a certain place in the series where the term has the minimum
absolute value (after that, the series is unusable), and the error of the
approximation cannot be smaller than that term.

*A error function $Erf(x)$!by asymptotic series 
*A asymptotic series!estimate of precision

For example, take the above asymptotic series for $Erfc(x)$.
The logarithm of the absolute value of the $n$-th term can be estimated using Stirling's formula for the factorial as
$$ Ln((2*n-1)!! /(2*x^2)^n) <=> n*(Ln(n)-1-2*Ln(x)) $$.
This function of $n$ has its minimum at $n=x^2$ where it is equal to $-x^2$.
Therefore the best we can do with this series is to truncate it before this term.
The resulting approximation to $Erfc(x)$ will have relative precision of order $Exp(-x^2)$.
Suppose that $x$ is large and we need to compute $Erfc(x)$ with $P$ decimal digits of floating point.
Then it follows that we can use the asymptotic series if $x>Sqrt(P*Ln(10))$.

We find that for a given finite $x$, no matter how large, there is a maximum
precision that can be achieved with the asymptotic series; if we need more
precision, we have to use a different method.

However, sometimes the function we are evaluating allows identity transformations that relate $f(x)$ to $f(y)$ with $y>x$.
For example, the Gamma function satisfies $x*Gamma(x)=Gamma(x+1)$.
In this case we can transform the function so that we would need to
evaluate it at  large enough $x$ for the asymptotic series to give us
enough precision.

		The AGM sequence algorithms

*A AGM sequence

Several algorithms are based on the arithmetic-geometric mean (AGM)
sequence. If one takes two numbers $a$, $b$ and computes their arithmetic
mean $(a+b)/2$ and their geometric mean $Sqrt(a*b)$, then one finds that the two means
are generally much closer to each other than the original numbers. Repeating
this process creates a rapidly converging sequence of pairs.

More formally, one can define the function of two
arguments $AGM(x,y)$ as the limit of the sequence $a[k]$ where
$a[k+1]=1/2*(a[k]+b[k])$, $b[k+1]=Sqrt(a[k]*b[k])$, and the initial values
are $a[0]=x$, $b[0]=y$. (The limit of the sequence $b[k]$ is the same.)
This function is obviously linear, $AGM(c*x, c*y)=c*AGM(x,y)$, so in
principle it is enough to compute $AGM(1,x)$ or arbitrarily select $c$ for convenience.

*A AGM sequence!integral representation

Gauss and Legendre knew that
the limit of the AGM sequence is related to the complete elliptic integral,
$$ Pi/2*1/AGM(a,Sqrt(a^2-b^2)) = Integrate(x,0,Pi/2)1/Sqrt(a^2-b^2*Sin(x)^2) $$.
(Here $0<b<a$.) This integral can be rearranged to provide some other useful functions. For
example, with suitable parameters $a$ and $b$, this integral is equal to
$Pi$.
Thus, one obtains a fast method of computing $Pi$ (the Brent-Salamin method).

The AGM sequence is also defined for complex values $a$, $b$.
One needs to take a square root $Sqrt(a*b)$, which requires a branch cut to be well-defined.
Selecting the natural cut along the negative real semiaxis ($Re(x)<0$, $Im(x)=0$), we obtain an AGM sequence that converges for any initial values $x$, $y$ with positive real part.

*A AGM sequence!convergence rate

Let us estimate the convergence rate of the AGM sequence starting from $x$, $y$, following the paper [Brent 1975]. Clearly the worst case is when
the numbers $x$ and $y$ are very different (one is much larger than another). In this case
the numbers $a[k]$, $b[k]$ become approximately equal after about
$k=1/Ln(2)*Ln(Abs(Ln(x/y)))$ iterations (note: Brent's paper online mistypes
this as $1/Ln(2)*Abs(Ln(x/y))$).
This is easy to see: if $x$ is much larger than $y$, then at each step the ratio $r:=x/y$ is transformed into $r'=1/2*Sqrt(r)$.
When the two numbers become roughly equal to each other, one needs about $Ln(n)/Ln(2)$ more
iterations to make the first $n$ (decimal) digits of $a[k]$ and $b[k]$
coincide, because the relative error $epsilon=1-b/a$ decays approximately as
$epsilon[k]<=>1/8*Exp(-2^k)$.

Unlike Newton's iteration, the AGM sequence does not correct errors, so all numbers need to be computed with full precision.
Actually, slightly more precision is needed to compensate for accumulated round-off error.
Brent (in [Brent 1975]) says that $O(Ln(Ln(n)))$ bits of accuracy are lost to round-off error if there are total of $n$ iterations.

The AGM sequence can be used for fast computations of $Pi$, $Ln(x)$ and $ArcTan(x)$.
However, currently the limitations of Yacas internal math make these methods less efficient than simpler methods based on Taylor series and Newton iterations.

		The binary splitting method

*A binary splitting

The method of binary splitting is well explained in [Haible <i>et al.</i> 1998].
Some examples are also given in [Gourdon <i>et al.</i> 2001].
This method applies to power series of rational numbers and to hypergeometric series.
Most series for transcendental functions belong to this category.

If we need to take $O(P)$ terms of the series to obtain $P$ digits of precision, then ordinary methods would require $O(P^2)$ arithmetic operations.
(Each term needs $O(P)$ operations because all coefficients are rational numbers with $O(P)$ digits and we need to perform a few short multiplications or divisions.)
The binary splitting method replaces the ordinary methods with $O(P^2)$ operations by $O(M(P*Ln(P))*Ln(P))$ operations.
In other words, we need to perform long multiplications of integers of size $O(P*Ln(P))$ digits, but we need only $O(Ln(P))$ such multiplications.
The binary splitting method performs better than the straightforward summation method if the cost of multiplication is smaller than $O(P^2)/Ln(P)$.

Thus there are two main limitations of the binary splitting method:
*	we can only compute functions of small integer or rational arguments;
*	the method is faster only at high enough precision, when advanced multiplication methods become more efficient than simple $O(P^2)$ methods.

Some advantages of the method are:
*	the method is asymptotically fast and, when applicable, outperforms most other methods at very high precision;
*	the method uses exact integer arithmetic, so one can store exact intermediate results of the calculation for checkpointing and parallelization, and also one can later resume the summation to get more precision.

	    Description of the method

We follow [Haible <i>et al.</i> 1998].
The method applies to any series of rational numbers of the form
$$ S = Sum(n,0,N-1, A(n)/B(n)) $$,
where $A$, $B$ are integer coefficients with $O(n*Ln(n))$ bits.
Usually the series is of the particular form
$$ S(0,N) := Sum(n,0,N-1, a(n)/b(n)*(p(0)* ... *p(n))/(q(0)* ... *q(n))) $$,
where $a$, $b$, $p$, $q$ are polynomials in $n$ with small integer coefficients and values that fit into $O(Ln(n))$ bits.

For example, the Taylor series for $ArcSin(x)$ is of this form:
$$ ArcSin(x) = x + 1/2*x^3/3+(1*3)/(2*4)*x^5/5+(1*3*5)/(2*4*6)*x^7/7 + ...$$
This example is of the above form with the definitions $a=1$, $b(n)=2*n+1$, $p(n)=x^2*(2*n-1)$, $q(n)=2*n$ for $n>=1$ and $p(0)=x$, $q(0)=1$.
(The method will apply only if $x$ is a rational number.)

The goal is to compute the sum $S(0,N)$ with a chosen number of terms $N$.
Instead of computing the rational number $S$, the binary splitting method computes the following four integers $P$, $Q$, $B$, and $T$:
$$P(0,N) := p(0)* ... *p(N-1)$$,
$$Q(0,N) := q(0)* ... *q(N-1)$$,
$$B(0,N) := b(0) * ... *b(N-1)$$, and
$$T(0,N) := B(0,N)*Q(0,N)*S(0,N)$$.
At first sight it seems difficult to compute $T$, but the computation is organized recursively.
These four integers are computed for the left ($l$) half and for the right ($r$) half of the range [$0$, $N$) and then combined using the obvious recurrence relations $P=P[l]*P[r]$, $Q=Q[l]*Q[r]$, $B=B[l]*P[r]$, and the slightly less obvious relation
$$T=B[r]*Q[r]*T[l]+B[l]*P[l]*T[r]$$.
Here we used the shorthand $P[l] := P(0,N/2-1)$, $P[r] := P(N/2, N-1)$ and so on.

Thus the range [$0$, $N$) is split in half on each step.
At the base of recursion the four integers $P$, $Q$, $B$, and $T$ are computed directly.
At the end of the calculation (top level of recursion), one floating-point division is performed to recover $S=T/(B*Q)$.
It is clear that the four integers carry the full information needed to continue the calculation with more terms.
So this algorithm is easy to checkpoint and parallelize.

The integers $P$, $Q$, $B$, and $T$ grow during the calculation to $O(N*Ln(N))$ bits, and we need to multiply these large integers.
However, there are only $O(Ln(N))$ steps of recursion and therefore $O(Ln(N))$
long multiplications are used.
If the series converges linearly, we need $N=O(P)$ terms to obtain $P$ digits of precision.
Therefore, the total asymptotic cost of the method is $O(M(P*Ln(P))*Ln(P))$ operations.

A more general form of the binary splitting technique is also given in [Haible <i>et al.</i> 1998].
In particular cases the algorithm may be simplified.

*REM FIXME: fill in the details and give examples


