		Designing modules in the Yacas scripting language

	    Introduction

For any software project where the source code grows to
a substantial amount of different modules, there needs to be
a way to define interfaces between the modules, and a way
to make sure the modules don't interact with the environment
in an unintended way. 

One hallmark of a mature programming language is that it 
supports modules, and a way to define its interface while
hiding the internals of the module. This section describes
the mechanisms for doing so in the Yacas scripting language.

	    Demonstration of the problem
 
Unintentional interactions between two modules typically happen 
when the two modules accidentally share a common "global"
resource, and there should be a mechanism to guarantee that this
will not happen.

The following piece of code is a little example that demonstrates
the problem:

	SetExpand(fn_IsString) <-- [expand:=fn;];
	ram(x_IsList)_(expand != "") <-- ramlocal(x);
	expand:="";
	ramlocal(x) := Map(expand,{x});

This little bit of code defines a function {ram} that calls the
function {Map}, passing the argument passed if it is a string, and
if the function to be mapped was set with the {SetExpand} function.
It contains the following flaws:

*	0. {expand} is a global variable with a rather generic name, one
that another module might decide to use.
*	0. {ramlocal} was intended to be used from within this module only, and
doesn't check for correctness of arguments (a small speed up optimization
that can be used for routines that get called often). As it is, it can be
called from other parts, or even the command line.
*	0. the function {ramlocal} has one parameter, named {x}, which is also
generic (and might be used in the expression passed in to the function),
and {ramlocal} calls {Map}, which calls {Eval} on the arguments. 

The above code can be entered into a file and loaded from the command
line at leisure. Now, consider the following command line interaction
after loading the file with the above code in it:

	In> ramlocal(a)         
	In function "Length" : 
	bad argument number 1 (counting from 1)
	Argument matrix[1] evaluated to a
	In function call  Length(a)
	CommandLine(1) : Argument is not a list

We called {ramlocal} here, which should not have been allowed.

	In> ram(a)
	Out> ram(a);

The function {ram} checks that the correct arguments are passed in
and that {SetExpand} was called, so it will not evaluate if these
requirements are not met. 

Here are some lines showing the functionality of this code as
it was intended to be used:

	In> SetExpand("Sin")
	Out> "Sin";
	In> ram({1,2,3})
	Out> {Sin(1),Sin(2),Sin(3)};

The following piece of code forces the functionality to break
by passing in an expression containing the variable {x}, which
is also used as a parameter name to {ramlocal}.

	In> ram({a,b,c})
	Out> {Sin(a),Sin(b),Sin(c)};
	In> ram({x,y,z})
	Out> {{Sin(x),Sin(y),Sin(z)},Sin(y),Sin(z)};

This result is obviously wrong, comparing it to the call above.
The following shows that the global variable {expand} is exposed
to its environment:

	In> expand
	Out> "Sin";


	    Declaring resources to be local to the module

The solution to the problem is {LocalSymbols}, which changes every
symbol with a specified name to a unique name that could never
be entered by the user on the command line and guarantees that it
can never interact with the rest of the system. The following code
snippet is the same as the above, with the correct use of {LocalSymbols}:


	LocalSymbols(x,expand,ramlocal) [
	  SetExpand(fn_IsString) <-- [expand:=fn;];
	  ram(x_IsList)_(expand != "") <-- ramlocal(x);
	  expand:="";
	  ramlocal(x) := Map(expand,{x});
	];


This version of the same code declares the symbols {x}, {expand}
and {ramlocal} to be local to this module.

With this the interaction becomes a little bit more predictable:

	In> ramlocal(a)
	Out> ramlocal(a);
	In> ram(a)
	Out> ram(a);
	In> SetExpand("Sin")
	Out> "Sin";
	In> ram({1,2,3})
	Out> {Sin(1),Sin(2),Sin(3)};
	In> ram({a,b,c})
	Out> {Sin(a),Sin(b),Sin(c)};
	In> ram({x,y,z})
	Out> {Sin(x),Sin(y),Sin(z)};
	In> expand
	Out> expand;


	    When to use and when not to use {LocalSymbols}

The {LocalSymbols} should ideally be used for every global variable,
for functions that can only be useful within the module and thus
should not be used by other parts of the system, 
and for local variables that run the risk of being passed into 
functions like {Eval}, {Apply}, {Map}, etcetera (functions that
re-evaluate expressions).

A rigorous solution to this is to make all parameters to functions
and global variables local symbols by default, but this might cause
problems when this is not required, or even wanted, behaviour.

The system will never be able to second-guess which function
calls can be exposed to the outside world, and which ones should
stay local to the system. It also goes against a design rule of Yacas:
everything is possible, but not obligatory. This is important 
at moments when functionality is not wanted, as it can be hard 
to disable functionality when the system does it automatically.

There are more caveats: if a local variable is made unique with 
{LocalSymbols}, other routines can not reach it by using the 
{UnFence} construct. This means that {LocalSymbols} is not always 
wanted. 

Also, the entire expression on which the {LocalSymbols} command works
is copied and modified before being evaluated, making loading
time a little slower. This is not a big problem, because the 
speed hit is usually during calculations, not during loading, but
it is best to keep this in mind and keep the code passed to 
{LocalSymbols} concise.








		How Yacas Deals With Sets of Solutions

	    Introduction

*REM @@@@@@@@@@@@@@@@@@@@@@@@
THIS IS A DRAFT, THE FUNCTIONALITY DESCRIBED BELOW IS NOT FULLY
IMPLEMENTED YET!!! THIS IS JUST A PROPOSAL FOR HOW SOLVE MIGHT
WORK.

*HEAD (This is a draft)

Worries: 

*	need to change all code that uses Solve
*	need a lot of changes in documentation
*	arguments to solve are a bit more verbose than the previous
version: Solve({eq1,eq2},vars) versus Solve(eq1 And eq2,vars). Suddenly
things like Solve(leftlist==rightlist,vars) is not possible any more.
This has to be done with extra commands (which is ok?). It can not
stay the way it was, because lists now mean something else, a collection
of disjunct solutions.

*REM @@@@@@@@@@@@@@@@@@@@@@@@

The difference between a problem stated and a solution given is
a subtle one. From a mathematical standpoint, 

	In> Integrate(x,0,B)Cos(x)
	Out> Sin(B);

And thus

	Integrate(x,0,B)Cos(x) == Sin(B)

is a true statement. Furthermore, the left hand side is mathematically
equivalent to the right hand side. Working out the integration, to
arrive at an expression that doesn't imply integration any more is
generally perceived to be a more desirable result, even though
the two sides are equivalent mathematically.

This implies that the statement of a set of equations declaring
equalities is on a same footing as the resulting equations stating
a solution: 

$$ a*x+b==c => x==(c-b)/a $$. 

If the value of x is needed, the right hand side is more desirable.

Viewed in this way, the responsibility of a {Solve} function could
be to manipulate a set of equations in such a way that a certain
piece of information can be pried from it (in this case the value
of $ x == x(a,b,c) $.

A next step is to be able to use the result returned by a {Solve}
operation. 

	    Implementation Semantics of Solve in Yacas

Suppose there is a set of variables that has a specific combination
of solutions and these solutions need to be filled in in an expression:
the {Where} operator can be used for this:

	In> x^2+y^2 Where x==2 And y==3
	Out> 13;

Solve can return one such solution tuple, or a list of tuples.
The list of equations can be passed in to Solve in exactly the same
way. Thus:

	In> Solve(eq1,var)
	Out> a1==b1;
	In> Solve(eq1 And eq2 And eq3,varlist)
	Out> {a1==b1 And a2==b2,a1==b3 And a2==b4};

These equations can be seen as simple simplification rules, the
left hand side showing the old value, and the right hand side
showing the new value. Interpreted in that way, {Where} 
is a little simplifier for expressions, using values found by Solve.

Assigning values to the variables values globally can be handled with
an expression like

	solns := Solve(equations,{var1,var2});
	{var1,var2} := Transpose({var1,var2} Where solns);

Multiple sets of values can be applied:

	In> x^2+y^2 Where {x==2 And y==2,x==3 And y==3}
	Out> {8,18};

This assigns the the variables lists of values. These variables
can then be inserted into other expressions, where threading will
fill in all the solutions, and return all possible answers.
	
Groups of equations can be combined, with

	Equations := EquationSet1 AddTo EquationSet2

or, 

	Equations := Equations AddTo Solutions;

Where {Solutions} could have been returned by {Solve}. This last
step makes explicit the fact that equations are on a same footing,
mathematically, as solutions to equations, and are just another
way of looking at a problem.

The equations returned can go farther in that multiple solutions
can be returned: if the value of $ x $ is needed and the equation
determining the value of $ x $ is $ x := Abs(a) $, then a set
of returned solutions could look like:

	Solutions := { a>=0 And x==a, a<0 And x== -a }

The semantics of this list is:

	either a >= 0 And x equals a, or
	       a < 0 And x equals -a


When more information is published, for instance the value
of $ a $ has been determined, the sequence for solving this
can look like:

	In> Solve(a==2 AddTo Solutions,{x})
	Out> x==2;

The solution {a<0 And x==-a} can not be satisfied, and thus is removed
from the list of solutions.

Introducing new information can then be done with the AddTo
operator:

	In> Solutions2 := (a==2 AddTo Solutions);
	Out> { a==2 And a>=0 And x==a, a==2
	  And a<0 And x==-a };

In the above case both solutions can not be true any more, and thus
when passing this list to {Solve}:

	In> Solve(Solutions2,{x})
	Out> x==2;

{AddTo} combines multiple equations through a tensor-product like
scheme:

	In> {A==2,c==d} AddTo {b==3 And  d==2}
	Out> {A==2 And b==3 And d==2,c==d
	  And b==3 And d==2};
	In> {A==2,c==d} AddTo {b==3, d==2}
	Out> {A==2 And b==3,A==2 And d==2,c==d
	  And b==3,c==d And d==2};

A list {a,b} means that a is a solution, OR b is a solution.
AddTo then acts as a AND operation:
	(a or b) and (c or d) => 
	(a or b) Addto (c or d) => 
	(a and c) or (a and d) or (b and c) or (b and d)


{Solve} gathers information as a list of identities. The second
argument is a hint as to what it needs to solve for. It can be a list
of variables, but also "Ode" (to solve ordinary differential equations),
"Trig" (to simplify for trigonometric identities), "Exp" to simplify
for expressions of the form $ Exp(x) $, or "Logic" to simplify
expressions containing logic. The "Logic" simplifier also should
deal with $ a > 2 And a < 0 $ which it should be able to reduce 
to {False}.

{Solve} also leaves room for an 'assume' type mechanism, where the
equations evolve to keep track of constraints. When for instance
the equation $ x == Sin(y) $ is encountered, this might result
in a solution set 

	y == ArcSin(x) And x>=-1 And x <= 1


	    Use Case Scenarios

*REM TODO@@@@ THIS SECTION IS GOING TO CONTAIN EXPLICIT EXAMPLES OF
SOLVING INTERESTING SETS OF EQUATIONS, AND SHOWING THAT THE SOLVE
SCHEME IS ACTUALLY EASY TO USE.

*HEAD To be filled in


			The Yacas internal numeric library

		Introduction

Although there are quite good free arbitrary precision arithmetic
libraries out there, {Yacas} comes with its own (default) implementation.
Other libraries can be used too, but the addition of a small native 
arithmetic library reduces dependencies on other packages.

This part describes how the arithmetic library is embedded into
{Yacas}, and how to embed other arithmetic libraries.

		The link between the interpreter and the arithmetic library

The interpreter (as of version 1.0.54) has the concept of an <i>atom</i>, an object
which has a string representation. As soon as a calculation needs
to be performed, the string representation is used to construct
an object representing the number, in an internal representation that
the arithmetic library can work with.

The basic layout is as follows: there is a base class {NumberBase},
which is a pure abstract class.
This class offers basic numerical functions,
arithmetic operations such as addition and multiplication, through a set of pure virtual methods.

Integers and floating-point numbers are to be handled by the same class.

An actual number class will be a subclass of {NumberBase} that implements a particular internal representation of numbers and 
Yacas will usually be compiled to use one specific number library. This library has to be hidden
behind the class {BigNumber}, which is derived from {NumberBase}.

Theoretically, it is still possible to offer more than one library, by hiding it behind another
{NumberBase}-derived class, which dispatches calculation requests to the right library, which
could be configured at run time (if this were enabled at compile time).

This setup allows different arithmetic libraries to be used;
each library has to be hidden behind a sub-class of {NumberBase}.

The number object should be able to render itself back to a string,
which can then be passed back to the interpreter as a result, if so needed.

		Interface to the objects

The following code demonstrates how to use the number objects.

	// Calculate z=x+y where x=10 and y=15
	BigNumber x("10",100,10);
	BigNumber y("15",100,10);
	BigNumber z;
	z.Add(x,y,10));    
	// cast the result to a string
	LispString  str;
	z.ToString(str,10);

The behaviour is such that in the above example z will contain the result of adding x and
y, without modifying x or y. One situation might occur where a calculation might modify
one of its arguments. This might happen when one argument passed in is actually the 
object performing the calculation itself. For example, if a calculation

	x.Add(x,y);
were issued, the result would be assigned to x, so the old 
value x contained would be disposed any way. In this case
the specific implementation might opt to perform the operation
destructively. Some operations can be done a lot more efficiently
when destructive operations are allowed. Among them are 
for example {Negate}, {Add}, {ShiftLeft}, {ShiftRight}, etcetera,
for which efficient implementations might exist if the operation
can be done in-place. When this is the case, a <i>copy</i>
operation can be saved.

The destructive operation should however never have side effects
that differ from an implementation that works by copying 
objects before performing tasks on them. For instance, if the
specific object representing the number inside the numeric class
is shared with other objects, it should not allow the destructive
operation, as then other objects might start behaving differently.


		Interface definition

The basic arithmetic class {BigNumber} defines some simple arithmetic operations,
through which other more elaborate functions can be built.
Particular implementations of the multiple-precision library will be wrapped by the {BigNumber} class, and the rest of the Yacas core should only use the {BigNumber} API and should be ignorant of those implementations.

This API will not be completely exposed to Yacas scripts, because some of these functions are too low-level.
(For the functions that seem to be useful for Yacas, suggested Yacas bindings are given below.)  
Among the low-level functions, only those that are very useful for optimization will be available to the Yacas scripts.
But the full API will be available to C++ plugins, so that multiple-precision algorithms could be efficiently implemented when performance is critical.
Intermediate-level arithmetic functions such as {MathAdd}, {MathDiv}, {MathMod} and so on could be implemented either in the Yacas core or in plugins, through this low-level API.
The library scripts will be able to transform numerical expressions such as {x:=y+z} into calls of these intermediate-level functions.


*A multiple-precision facility!requirements

Here we list the basic arithmetic operations that need to be implemented by a multiple-precision class {BigNumber}.
The operations are divided into several categories for convenience.
Equivalent Yacas script code is given, as well as examples of C++ usage.

1. Input/output operations.
*	Construct a number from a string in given base.
The format is the standard integer, fixed-point and floating-point representations of numbers.
When the string does not contain the period character "{.}" or the exponent character "{e}" (the exponent character "{@}" should be used for $base>10$),
the result is an integer number and the precision argument is ignored.
Otherwise, the result is a floating-point number
rounded to a given number of <i>base digits</i>.
C++:
	x.SetTo("2.e-19", 100, 10);
Here we encounter a problem of ambiguous hexadecimal exponent:
	x.SetTo("2a8c.e2", 100, 16);
It is not clear whether the above number is in exponential notation or not.
But this is hopefully not a frequently encountered situation.
We assume that the exponent character for $base>10$ is "{@}" and not "{e}".
*	Print a number to a string in a given precision and in a given base.
The precision is given as the number of digits in the given base.
The value should be rounded to that number of significant base digits.
(Integers are printed exactly, regardless of the given precision.)
C++:
	x.ToString(buffer, 200, 16); // hexadecimal
	x.ToString(buffer, 40, 10); // decimal
*	Construct a number from a platform number (a 32-bit integer or a double precision value).
C++:
	x.SetTo(12345); y.SetTo(-0.001);
*	Obtain an approximate representation of {x} as double-precision value.
(The conversion may cause overflow or underflow, in which case the result is undefined.)
C++:
	double a=x.Double();

2. Basic object manipulation. These operations, as a rule, do not need to change the numerical value of the object.
*	Copy a number, {x := y}.
This operation should copy the numerical value exactly, without change.
C++:
	x.SetTo(y);
*	Compare two numbers for equality, {x = y}.
C++:
	x.Equals(y)==true;
Yacas:
	MathEquals(x,y)
Note that the values are compared arithmetically, their internal precision may differ, and integers may be compared to floats.
It is only guaranteed that {Equals} returns true for equal integers, for an integer and a floating-point number with the same integer value, and for two exactly bit-by-bit equal floating-point numbers.
Floating-point comparison may be unreliable due to roundoff error and particular internal representations.
So it may happen that after {y:=x+1;} {y:=y-1;} the comparison
	y.Equals(x)
will return {false}.
*	Check whether the number {x} is of integer or floating type.
(Both types are represented by the same class {BigNumber}, and we need to be able to distinguish them.)
C++:
	x.IsInt()==true;
*	Check whether the number {x} has an integer value. (Not necessarily the same as the previous function, because a floating-point type can also have an integer value.)
C++:
	x.IsIntValue()==true;
Yacas: part of the implementation of
	IsInteger(x)
*	Change the type of a number from integer to float without changing the numerical value. Change the type from float to integer, rounding off if necessary.
C++:
	x.BecomeInt(); x.BecomeFloat();

3. Basic arithmetic operations.
Note that here "precision" always means the number of significant <i>bits</i>, i.e. digits in the base 2, <i>not decimal digits</i>.
*	Compare two objects, {x<y}. Returns {true} if the numerical comparison holds, regardless of the value types (integer or float).
C++:
	x.LessThan(y)==true;
Yacas:
	LessThan(x,y)
*	Compute the integer part of a number, {x := Floor(y)}.
This function should round toward arithmetically smaller integers, as usual.
C++:
	x.Floor(y);
Yacas:
	MathFloor(x)
*	Report the current precision of a number {x} in bits.
C++:
	prec=x.GetPrecision();
Yacas:
	GetPrecision(x)
Every floating-point number contains information about how many significant bits of mantissa it currently has.
(A particular implementation may hold more bits for convenience but they may not be guaranteed to be correct.)
Integer numbers are always exact and do not have a concept of precision.
This function is undefined on integers.
The precision of a given number is changed automatically by arithmetic operations (due to round-off errors), by conversions from strings (to the given precision), or manually by the function {Precision}.
It is not strictly guaranteed that {Precision} returns the number of correct digits.
Rather, this number of digits is intended as an upper limit of the real achieved precision and as a rough measure of round-off errors.
(It is difficult to accurately track the round-off errors accumulated after many operations, without a time-consuming interval arithmetic or another similar technique.)
*	Set the precision of a number {x} and truncate (or expand) it to a given floating-point precision of {n} bits.
This has an effect of converting the number to the floating-point type with {n} significant bits of mantissa.
(No effect on integers.)
Note that the {Floor} function is not similar to {Precision} because
1) {Floor} always converts to an integer value while {Precision} converts generally to a floating-point value,
2) {Floor} always decreases the number while {Precision} tries to find the closest approximation.
For example, if $x= -1123.38$, then {x.Precision(1)} should return {-1024.}, since that's the best one-bit floating-point approximation.
However, {Floor(-1123.38)} returns {-1124}.
C++:
	x.Precision(300);
Yacas:
	Precision(x, 300)
*	Add two numbers, {x := y+z}, at given precision.
C++:
	x.Add(y,z, 300);
Yacas:
	MathAdd(x,y)
When subtracting almost equal numbers, a loss of precision will occur.
*	Negate a number, {x := -y}.
C++:
	x.Negate(y);
Yacas:
	MathNegate(x)
*	Multiply two numbers, {x := y*z}, at given precision.
C++:
	x.Multiply(y,z, 300);
Yacas:
	MathMultiply(x,y)
*	Divide two numbers, {x := y/z}, at given precision.
(Integers are divided exactly as integers and the "precision" argument is ignored.)
C++:
	x.Divide(y,z, 300);
Yacas:
	MathDivide(x,y)

4. Auxiliary operations (useful for optimization purposes but can be performed using the basic arithmetic).
All these operations are efficient to implement with a binary-based internal representation of big numbers.
*	Check whether the number {x} fits into a platform type {long} or {double}. (Optimization of comparison.)
Sometimes this helps avoid unnecessary calculations with big numbers.
Note that the semantics of this operation is different for integers and for floats. An integer is "small" only when it fits into a platform {long} integer.
A float is "small" when it can be approximated by a platform {double} (that is, when its exponent is not too large).
C++:
	x.IsSmall()==true;
Yacas:
	MathIsSmall(x)
*	Multiply two numbers and add to the third, {x := x+y*z}, at given precision. (Optimization of a frequently used operation.)
C++:
	x.MultiplyAdd(y,z, 300);
Yacas:
	MathMultiplyAdd(x,y,z)
*	Obtain the remainder modulo an integer, {x:=Mod(y,n)}.
C++:
	x.Mod(y,n);
Yacas:
	MathMod(x,n)
(Optimization of integer division, important for number theory applications.)
The integer modulus {n} is a big number.
The function is undefined for floating-point numbers.
*	Obtain the sign of the number {x} (result is {-1}, {0} or {1}). (Optimization of comparison with 0.)
C++:
	int sign_of_x = x.Sign();
Yacas:
	MathSign(x)
*	Obtain
the "bit count" of the number {x} (more precisely, the integer part of the binary logarithm of the absolute value of {x}).
For integers, this function counts the significant bits, i.e. the number of bits needed to represent the integer.
(The bit count of 4 is 3.)
For floating-point numbers, this function should return the binary exponent of the number (with sign), like the integer output of the standard C function {frexp}.
Result is an integer number (a "big number").
More formally: if $n=BitCount(x)$, and $x!=0$, then $1/2 <= Abs(x)*2^(-n) < 1$.
The bit count of 0 is 0.
(Optimization of the binary logarithm.)
C++:
	x.BitCount(y);
Yacas:
	MathBitCount(x)
*	Bit-shift the number (multiply or divide by the $n$-th power of $2$), {x := y >> n}, {x := y << n}.
For integers, this operation can be efficiently implemented because it has hardware support.
For floats, this operation is usually also much more efficient than multiplication or division by 2 (cf. the standard C function {ldexp}).
(Optimization of multiplication and division by a power of 2.)
Note that the shift amount is a big number.
C++:
	x.ShiftLeft(y, n); x.ShiftRight(y, n);
Yacas:
	ShiftLeft(x,n); ShiftRight(x,n);
(Versions of these functions with {n} a platform {long} integer are also provided for efficiency.)
*	Perform bitwise arithmetic, like in C: {x = y&z}, {x = y|z}, {x = y^z}, {x = ~y}.
This should be implemented only for integers.
Integer values are interpreted as bit sequences starting from the least significant bit.
(Optimization of operations on bit streams and some arithmetic involving powers of 2.)
C++:
	x.BitAnd(y,z); x.BitOr(y,z);
	x.BitXor(y,z); x.BitNot(y);
Yacas:
	BitAnd(x,y); BitOr(y,z);
	BitXor(y,z); BitNot(y);

The API includes only the most basic operations.
All other mathematical functions such as power, logarithm, cosine and so on, can be efficiently implemented using this basic interface.

Note that generally the arithmetic functions will set the type of the resulting object to the type of the result of the operation.
For example, operations that only apply to integers ({Mod}, {BitAnd} etc.) will set the type of the resulting object to integer if it is a float.
The results of these operations on non-integer arguments are undefined.

		Implementation notes

	    Precision

In some arithmetic operations the working precision is given explicitly.
Then the result of the operation should be rounded to the given number of bits (the precision is always given in bits).
These operations (add, multiply, divide) should be nevertheless exact on integers, and the precision argument is then ignored.

Integers must grow or shrink when necessary.
Floating-point values only grow up to the given number of significant bits and then a round-off must occur.


There is an alternative to having to explicitly  specify precision for all operations.
The precision of arithmetic operations on floating-point numbers can be maintained automatically, in a version of "poor man's interval calculus".

Suppose we have two floating-point numbers $x$ and $y$ and we know that they have certain numbers of correct mantissa bits, say $m$ and $n$.
In other words, $x$ is an approximation to an unknown real number $x'=x*(1+delta)$ and we know that $Abs(delta)<2^(-m)$; and similarly for $y$: $y'=y*(1+epsilon)$ with $Abs(epsilon)<2^(-n)$.
Here $delta$ and $epsilon$ are the relative errors for $x$ and $y$ and are typically much smaller than $1$.

Suppose that every floating-point number comes with its number of significant digits, i.e. we symbolically represent the numbers as pairs {{x,m}} or {{y,n}}.
When we perform an arithmetic operation on numbers, we need to update the precision component as well.
Now we shall consider the basic arithmetic operations to see how the precision is updated.

*HEAD Multiplication

If we need to multiply $x$ and $y$, the correct answer is $x'*y'$ but we only know an approximation to it, $x*y$.
We can estimate the precision by $x'*y' = x*y*(1+delta)*(1+epsilon)$ and it follows that the relative precision is at most $delta+epsilon$.
But we only represent the relative errors by the number of bits.
So we can either set the precision of $x*y$ to the smallest of the precisions of $x$ and $y$, or double it.

More formally, we have the estimates $Abs(delta)<2^(-m)$, $Abs(epsilon)<2^(-n)$ and we need a similar estimate $Abs(r)<2^(-p)$ for $r=delta + epsilon$.

If the two numbers $x$ and $y$ have the same number of correct bits, we should double the error (i.e. decrease the number of significant bits by 1).
But if they don't have the same number of bits, we cannot really estimate the error very well.
To be on the safe side, we might double the error if the numbers $x$ and $y$ have almost the same number of significant bits, and leave the error constant if the numbers of significant bits of $x$ and $y$ are very different.

The answer expressed as a formula is $p=Min(m,n)$ if $Abs(m-n)>1$ and $p=Min(m,n)-1$ otherwise.

*HEAD Division

Division is multiplication by the inverse number.
When we take the inverse of $x*(1+delta)$, we obtain approximately $1/x*(1-delta)$.
Therefore the precision does not change when we take the inverse.
So the handling of precision is exactly the same as for the multiplication.

*HEAD Addition

Addition is more complicated because the absolute rather than the relative precision plays the main role,
and because there may be roundoff errors associated with subtracting almost equal numbers.

Formally, we have the relative precision $r$ of $x+y$ as
$$ r = (delta*x+epsilon*y)/(x+y) $$.
We have the bounds on $delta$ and $epsilon$:
$$ [Abs(delta)<2^(-m); Abs(epsilon)<2^(-n); ] $$,
and we need to find a bit bound on $r$, i.e. an integer $p$ such that $Abs(r)<2^(-p)$.
But we cannot estimate $p$ without first computing $x+y$ and analyzing the relative magnitude of $x$ and $y$.
To perform this estimate, we need to use the bit counts of $x$ and $y$ and on $x+y$.
Let these bit counts be $a$, $b$ and $c$, so that $Abs(x)<2^(a+1)$, $Abs(y)<2^(b+1)$, and $2^c<=Abs(x+y)<2^(c+1)$.
(We assume that $x+y!=0$. Otherwise, we obtain an answer $0$ with $0$ correct bits of mantissa, that is, a total loss of precision due to cancellation.)
Now we can estimate $r$ as
$$ r <= Abs((x*2^(-m))/(x+y)) + Abs((y*2^(-n))/(x+y)) <= 2^(a+1-m-c)+2^(b+1-n-c)$$.
This is formally similar to multiplying two numbers with $a+1-m-c$ and $b+1-m-c$ correct bits.
As in the case of multiplication, we may take the minimum of the two numbers, or double one of them if they are almost equal.

*HEAD Computations with a given target precision

Using these rules, we can maintain a bound on the numerical errors of all calculations.
But sometimes we know in advance that we shall not be needing any more than a certain number of digits of the answer,
and we would like to avoid an unnecessarily high precision and reduce the computation time.

In this case we can truncate one or both of the arguments to a smaller precision before starting the operation.
In both cases (the multiplication and the addition), the precision is given by a formula that involves a comparison of two binary exponents $2^(-g)$ and $2^(-h)$.
Here $g$ and $h$ are some integers that are easy to obtain during the computation, as we have seen above.
This comparison will immediately show which of the arguments dominates the error.

The ideal situation would be when one of these exponentials is much smaller than the other, but not very much smaller (that would be a waste of precision).
In other words, we should aim for $Abs(g-h)<8$ or so, where $8$ is the number of guard bits we would like to maintain.
(Generally it is a good idea to have at least 8 guard bits;
somewhat more guard bits do not slow down the calculation very much, but 200 guard bits would be surely an overkill.)
Then the number that is much more precise than necessary can be truncated.

For example, if we find that $g=250$ and $h=150$, then we can safely truncate $x$ to $160$ bits or so;
if, in addition, we need only $130$ bits of final precision,
then we could truncate both $x$ and $y$ to about $140$ bits.

Note that when we need to subtract two almost equal numbers, there will be a necessary loss of precision,
and it may be impossible to decide on the required target precision before performing the subtraction.

	    Large exponents

One proposed feature of the {BigNumber} API is the support for large exponents for floating-point numbers.
The idea is that a floating-point number $x$ is equivalent to two integers $M$, $N$ such that $x=M*2^N$. Here $M$ is the (denormalized) mantissa and $N$ is the (binary) exponent.
The integer $M$ must be a "big integer" that may represent thousands of significant bits.
Then it seems natural to make the exponent $N$ also a big integer and not impose some platform-dependent limitations on its size.
Implementing this idea will help avoid some cases of overflow and underflow, although one would not expect that most real-world calculations will be significantly affected.

The only concern with this scheme is efficiency of operations.
Arithmetic with floating-point numbers requires only very simple operations on their exponents (basically, addition and comparisons).
The {BigNumber} API can be implemented efficiently so that there is not much loss of time when the numbers are actually small.
This will reduce the overhead required for handling exponents.

To realize this idea, the API specifies that big numbers are used in situations when normally one would expect a platform number.
The relevant functions are {BitCount}, {ShiftLeft}, {ShiftRight}.
At first sight, it seems unlikely that we shall need numbers with more than $2^32$ bits (although computer hardware may develop very quickly to cover that range).
However, the real catch is the floating-point overflow and underflow.
The bit count is defined as the integer part of the binary logarithm of the absolute value of the number.
Defined in this way, the "bit count" of a floating-point number can exceed a 32-bit {long} value; consider for example $x=Exp(Exp(1000))$.
Similarly, we may need to multiply a floating-point number by a very large power of $2$ using the very efficient {ShiftLeft} function.

On the other hand, it is impractical to print several billion digits to the screen.
So the precision argument in string printing functions should be not a big integer but a platform integer.
Also, alternative versions of the API functions {BitCount}, {ShiftLeft}, {ShiftRight} may be provided with platform-typed arguments.


In the future this limitation might be avoided if we use a 64-bit platform.
(Clearly, a 64-bit platform is a better choice for heavy-duty multiple-precision computations than a 32-bit platform.)


	    Library versions of mathematical functions

It is usually the case that a multiple-precision library implements some basic mathematical functions such as the square root.
A library implementation may be already available and more efficient than an implementation using the API of the wrapper class {BigNumber}.
In this case it is desirable to wrap the library implementation of the mathematical function, rather than use a suboptimal implementation.
This could be done in two ways.

First, we recognize that we shall only have one particular numerical library linked with Yacas, and we do not have to compile our implementation of the square root if this library already contains a good implementation.
We can use conditional compilation directives ({#ifdef}) to exclude our square root code and to insert a library wrapper instead.
This scheme could be automated, so that appropriate {#define}s are automatically created for all functions that are already available in the given multiple-precision library, and the corresponding Yacas kernel code that uses the {BigNumber} API is automatically replaced by library wrappers.

Second, we might compile the library wrapper as a plugin, replacing the script-level square root function with a plugin-supplied function.
This solution is easier in some ways because it doesn't require any changes to the Yacas core, only to the script library.
However, the library wrapper will only be available to the Yacas scripts and not to the Yacas core functions.
The basic assumption of the plugin architecture is that plugins can provide new external objects and functions to the scripts, but plugins cannot modify anything in the kernel.
So plugins can replace a function defined in the scripts, but cannot replace a kernel function.
Suppose that some other function, such as a computation of the elliptic integral which heavily uses the square root, were implemented in the core using the {BigNumber} API.
Then it will not be able to use the square root function supplied by the plugin because it has been already compiled into the Yacas kernel.

Third, we might put all functions that use the basic API ({MathSqrt}, {MathSin} etc.) into the script library and not into the Yacas kernel.
When Yacas is compiled with a particular numerical library, the functions available from the library will also be compiled as the kernel versions of {MathSqrt}, {MathPower} and so on
(using conditional compilation or configured at build time).
Since Yacas tries to call the kernel functions before the script library functions, the available kernel versions of {MathSqrt} etc. will supersede the script versions, but other functions such as {BesselJ} will be used from the script library.
The only drawback of this scheme is that a plugin will not be able to use the faster versions of the functions, unless the plugin was compiled specifically with the requirement of the particular numerical library.

So it appears that either the first or the third solution is viable.





	    The internal storage of BigNumber objects

An object of type {BigNumber} represents a number (and contains all
information relevant to the number), and offers an interface to
operations on it, dispatching the operations to an underlying
arbitrary precision arithmetic library.

Higher up, Yacas only knows about objects derived from {LispObject}.
Specifically, there are objects of class {LispAtom} which represent
an atom. They are uniquely represented by the result returned by
the {String()} method. 

For numbers, there is a separate class, {LispNumber}. Objects
of class {LispNumber} also have a {String()} method in case
a string representation of a number is needed, but the main 
uniquely identifying piece of information is the object of
class {BigNumber} which is returned by the {Number(precision)}
method.

The life cycle of a {LispNumber} is as follows:

*	1. A {LispNumber} can be born when the parser reads in a numeric
atom. In such a case an object of type {LispNumber} is created in stead
of the {LispAtom}. The {LispNumber} constructor proceeds to create
an object of type {BigNumber} in response.
*	1. For a calculation, say addition, two arguments are passed in,
and their internal object should be of class {LispNumber}, so that the
function doing the multiplication can get at the {BigNumber} objects
by calling the {Number()} methods. The function doing the multiplying
then creates a new {BigNumber}, stores the result of the multiplication
in it, and again creates a {LispNumber} by constructing it with the
resulting {BigNumber}. Note no conversion to string representation is needed,
and thus not made. The result is a {LispNumber} with just a {BigNumber}
inside it. Other operations can proceed to use this {BigNumber}
stored inside the {LispNumber}. This is in effect the second way a
{LispNumber} can be born.
*	1. Right at the end, when a result needs to be printed to screen,
the printer will call the {String()} method of the {LispNumber} object,
to get a string representation to print. The string representation is also
stored in the {LispNumber} This is not expensive, as it is just a pointer
to a string in the string container (hash table).

In order to fully support the {LispNumber} object, the function in the
kernel that determines if two objects are the same needs to know about
{LispNumber}. This is required to get valid behaviour. Pattern matching
for instance uses comparisons of this type, so comparisons are performed
often and need to be efficient.

The other functions working on numbers can, in principle, call the
{String()} method, but that induces conversions from {BigNumber}
to string, which are relatively expensive operations. For efficiency
reasons, the functions dealing with numeric input should call the
{Number()} method, operate on the {BigNumber} returned, and
return a {LispNumber} constructed with a {BigNumber}. A function
can call {String()} and return a {LispNumber} constructed with 
a string representation, but it will be less efficient.


*HEAD Precision tracking inside LispNumber

There are various subtle details when dealing with precision.
A number gets constructed with a certain precision, but a
higher precision might be needed later on. That is the reason there
is a {aPrecision} argument to the {Number()} method. 

A {BigNumber} gets constructed with a minimum lower bound on the 
precision, but the actual internal representation the underlying
numeric library uses might actually store the number with even
higher precision. This is needed because at a certain stage 
the actual digits it was constructed with need to be reproduced
to higher precision than when the object was created. The string
representation has to match the one the {BigNumber} was created with.

There are thus actually two precisions in the {BigNumber}. One is
the precision it was constructed with, the value passed in by the system.
This is the lower bound, the minimum precision the user requires.
Above that, the representation inside {BigNumber} has the actual
precision, the precision it needs to perform to specifications.
It has to reproduce the string representation it was constructed
with, so it might decide to use a higher precision than the system passed
to it.









